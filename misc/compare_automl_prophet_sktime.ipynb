{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Evaluation (Multiple Datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install automl_tool in editable mode\n",
    "\n",
    "# Locate project root (directory containing pyproject.toml) and install in editable mode\n",
    "import pathlib, sys, subprocess, shlex\n",
    "root = pathlib.Path.cwd()\n",
    "while root != root.parent and not (root / 'pyproject.toml').exists():\n",
    "    root = root.parent\n",
    "subprocess.check_call(shlex.split(f\"pip install -e {root}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prophet sktime statsmodels pandas_datareader --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from automl_tool.automl import AutoML\n",
    "from automl_tool.preprocessing import ts_train_test_split\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Helpers\n",
    "make_dates = lambda n, freq='ME', start='2000-01-01': pd.date_range(start, periods=n, freq=freq)\n",
    "\n",
    "def df_from_values(values, start='2000-01-01', freq='ME'):\n",
    "    return pd.DataFrame({'date': make_dates(len(values), freq, start), 'value': np.asarray(values, dtype=float)})\n",
    "\n",
    "def ensure_regular_frequency(df: pd.DataFrame, date_col: str = 'date', value_col: str = 'value') -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d[date_col] = pd.to_datetime(d[date_col])\n",
    "    d = d.sort_values(date_col).drop_duplicates(subset=[date_col])\n",
    "    # Try to infer frequency; fallback to median delta if needed\n",
    "    freq = pd.infer_freq(d[date_col])\n",
    "    if freq is None:\n",
    "        deltas = d[date_col].diff().dropna()\n",
    "        if len(deltas) == 0:\n",
    "            freq = 'D'\n",
    "        else:\n",
    "            td = deltas.median()\n",
    "            try:\n",
    "                freq = to_offset(td)\n",
    "            except Exception:\n",
    "                freq = 'D'\n",
    "    full_index = pd.date_range(d[date_col].iloc[0], d[date_col].iloc[-1], freq=freq)\n",
    "    d = d.set_index(date_col).reindex(full_index)\n",
    "    # Interpolate missing values over time; fallback to ffill then bfill\n",
    "    if d[value_col].isna().any():\n",
    "        try:\n",
    "            d[value_col] = d[value_col].interpolate(method='time')\n",
    "        except Exception:\n",
    "            d[value_col] = d[value_col].ffill().bfill()\n",
    "    d = d.reset_index().rename(columns={'index': date_col})\n",
    "    return d[[date_col, value_col]]\n",
    "\n",
    "# Synthetic dataset generators (diverse shapes)\n",
    "def synth_seasonal(n=1000):\n",
    "    t = np.arange(n)\n",
    "    y = 10*np.sin(2*np.pi*t/12) + 0.5*t + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_linear(n=1000):\n",
    "    t = np.arange(n)\n",
    "    y = 0.3*t + np.random.normal(0, 3, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_quadratic(n=1000):\n",
    "    t = np.arange(n)\n",
    "    y = 0.01*(t-n/2)**2 + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_logistic(n=1000):\n",
    "    t = np.arange(n)\n",
    "    midpoint = n/2\n",
    "    y = 100/(1+np.exp(-(t-midpoint)/10)) + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_walk(n=1000):\n",
    "    rng = np.random.default_rng(123)\n",
    "    y = np.cumsum(rng.normal(0.3, 1.0, n))\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_piecewise(n=1000):\n",
    "    t = np.arange(n)\n",
    "    k1 = int(n*0.3); k2 = int(n*0.65)\n",
    "    base = np.piecewise(t,\n",
    "                        [t < k1, (t >= k1) & (t < k2), t >= k2],\n",
    "                        [lambda x: 0.2*x,\n",
    "                         lambda x: (0.2*k1) + (-0.1)*(x-k1),\n",
    "                         lambda x: (0.2*k1) + (-0.1)*(k2-k1) + 0.4*(x-k2)])\n",
    "    y = base + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_spiky(n=1000):\n",
    "    rng = np.random.default_rng(0)\n",
    "    y = 20 + np.sin(2*np.pi*np.arange(n)/24) + rng.normal(0, 2, n)\n",
    "    idx = rng.choice(n, size=max(10, n//30), replace=False)\n",
    "    y[idx] += rng.uniform(10, 25, len(idx))\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_multiseason(n=1000):\n",
    "    t = np.arange(n)\n",
    "    y = 5*np.sin(2*np.pi*t/12) + 3*np.sin(2*np.pi*t/6) + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "# Collect datasets (n=1000 each synthetic)\n",
    "datasets = {\n",
    "    'Seasonal+Trend': synth_seasonal(n=1000),\n",
    "    'Linear Trend': synth_linear(n=1000),\n",
    "    'Quadratic': synth_quadratic(n=1000),\n",
    "    'Logistic (S-curve)': synth_logistic(n=1000),\n",
    "    'Random Walk (drift)': synth_walk(n=1000),\n",
    "    'Piecewise (changepoints)': synth_piecewise(n=1000),\n",
    "    'Spiky Intermittent': synth_spiky(n=1000),\n",
    "    'Multi-seasonal': synth_multiseason(n=1000),\n",
    "}\n",
    "\n",
    "# Add real datasets from statsmodels\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    # Sunspots (yearly)\n",
    "    try:\n",
    "        sun = sm.datasets.sunspots.load_pandas().data\n",
    "        df_sun = pd.DataFrame({\n",
    "            'date': pd.to_datetime(sun['YEAR'], format='%Y', errors='coerce'),\n",
    "            'value': sun['SUNACTIVITY'].astype(float)\n",
    "        }).dropna()\n",
    "        datasets['Sunspots'] = df_sun\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Mauna Loa CO2 (weekly)\n",
    "    try:\n",
    "        co2 = sm.datasets.co2.load_pandas().data\n",
    "        co2 = co2.copy()\n",
    "        if 'co2' in co2.columns and 'date' not in co2.columns:\n",
    "            co2 = co2.reset_index().rename(columns={'index': 'date', 'co2': 'value'})\n",
    "        else:\n",
    "            if 'date' not in co2.columns:\n",
    "                co2 = co2.reset_index().rename(columns={'index': 'date'})\n",
    "            if 'value' not in co2.columns:\n",
    "                first_val = [c for c in co2.columns if c != 'date'][0]\n",
    "                co2 = co2.rename(columns={first_val: 'value'})\n",
    "        co2 = co2[['date', 'value']].dropna()\n",
    "        datasets['CO2'] = co2\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Add selected FRED series\n",
    "try:\n",
    "    from pandas_datareader import data as pdr\n",
    "    fred_codes = ['CPIAUCSL', 'UNRATE', 'INDPRO']\n",
    "    for code in fred_codes:\n",
    "        try:\n",
    "            df_fred = pdr.DataReader(code, 'fred', start='1990-01-01')\n",
    "            df_fred = df_fred.rename(columns={code: 'value'}).reset_index().rename(columns={'DATE': 'date'})\n",
    "            df_fred = df_fred.dropna()\n",
    "            datasets[code] = df_fred\n",
    "        except Exception:\n",
    "            continue\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Filter to only the datasets the user wants\n",
    "allowed = {\n",
    "    'Seasonal+Trend', 'Linear Trend', 'Quadratic', 'Logistic (S-curve)',\n",
    "    'Random Walk (drift)', 'Piecewise (changepoints)', 'Spiky Intermittent',\n",
    "    'Multi-seasonal', 'Sunspots', 'CO2', 'CPIAUCSL', 'UNRATE', 'INDPRO'\n",
    "}\n",
    "datasets = {k: v for k, v in datasets.items() if k in allowed}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output directory for performance tables\n",
    "from pathlib import Path\n",
    "OUTPUT_DIR = Path.cwd() / 'output_data'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "winners = []\n",
    "winners_pipelines = []\n",
    "winners_automl_objs = []\n",
    "\n",
    "# Normalize to equidistant dates\n",
    "for k in list(datasets.keys()):\n",
    "    datasets[k] = ensure_regular_frequency(datasets[k], 'date', 'value')\n",
    "\n",
    "fdw, holdout_window, forecast_window = 18, 24, 1\n",
    "maes = []\n",
    "mapes = [] \n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if len(df) < fdw + holdout_window + forecast_window + 1:\n",
    "        continue\n",
    "    X, y = df, df['value']\n",
    "    X_train, X_holdout, y_train, y_holdout = ts_train_test_split(\n",
    "        X, y, 'value', 'date', fdw, holdout_window, forecast_window=forecast_window\n",
    ")\n",
    "    # Skip datasets that are too short for 5-fold TimeSeriesSplit with this test size\n",
    "    if len(X_train) <= 5 * holdout_window:\n",
    "        continue\n",
    "    automl_mod = AutoML(X_train, y_train, 'value', time_series=True)\n",
    "    automl_mod.fit_pipeline(holdout_window=holdout_window)\n",
    "    preds = automl_mod.fitted_pipeline.best_estimator_.predict(X_holdout)\n",
    "\n",
    "    mae = mean_absolute_error(y_holdout, preds)\n",
    "    mape = mean_absolute_percentage_error(y_holdout, preds) * 100\n",
    "    maes.append(mae)\n",
    "    mapes.append(mape)\n",
    "\n",
    "    print(f\"{name}: MAE={mae:.3f}, MAPE={mape:.2f}%\")\n",
    "\t# ... inside your datasets loop, after fit:\n",
    "    winners.append(type(automl_mod.fitted_pipeline.best_estimator_.get_params()['model']).__name__)\n",
    "    winners_pipelines.append(automl_mod.fitted_pipeline.best_estimator_)\n",
    "    winners_automl_objs.append(automl_mod)\n",
    "\n",
    "if maes:\n",
    "    print(f\"Average MAE: {float(np.mean(maes)):.3f}\")\n",
    "    print(f\"Average MAPE: {float(np.mean(mapes)):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_performance = pd.DataFrame({'series':datasets.keys(), 'mae_challenger':maes, 'mape_challenger':mapes})\n",
    "\n",
    "automl_performance#.to_csv(OUTPUT_DIR / 'automl_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('cmdstanpy')\n",
    "logger.handlers = []\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from itertools import product\n",
    "\n",
    "# Enhanced Prophet benchmarking block (with yearly-series safeguards + expanded hyperparameter grid)\n",
    "# Configuration\n",
    "MIN_TRAIN_POINTS = 30               # skip very short series\n",
    "LOG_TOL = 1e-6                      # small constant for log transform\n",
    "OUTLIER_Z = 4.0                     # z-score threshold for winsorization\n",
    "HYPERPARAM_GRID = {\n",
    "    'changepoint_prior_scale': [.005, 0.05, 0.5],\n",
    "    'seasonality_prior_scale': [5.0, 15.0],\n",
    "    'yearly_fourier': [10, 20],\n",
    "    'changepoint_range': [0.8, 0.9],\n",
    "    'seasonality_mode': ['additive', 'multiplicative'],\n",
    "    'growth': ['linear', 'logistic']  # logistic will auto-add cap/floor\n",
    "}\n",
    "ENABLE_MONTHLY = True\n",
    "ENABLE_QUARTERLY = True\n",
    "USE_LOG = True                      # per-series decision still checked\n",
    "ROBUST_TREND = True                 # remove extreme spikes before fit\n",
    "LOGISTIC_CAP_MULT = 1.15            # cap = max(train_y) * this\n",
    "LOGISTIC_FLOOR_MULT = 0.85          # floor = min(train_y) * this (if positive)\n",
    "\n",
    "prophet_maes = {}\n",
    "prophet_mapes = {}\n",
    "prophet_details = {}\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def _prepare_series(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = (df[['date','value']]\n",
    "          .rename(columns={'date':'ds','value':'y'})\n",
    "          .sort_values('ds')\n",
    "          .reset_index(drop=True))\n",
    "    s = s.drop_duplicates(subset='ds')\n",
    "    s['ds'] = pd.to_datetime(s['ds'])\n",
    "    s = s.dropna(subset=['y'])\n",
    "    return s\n",
    "\n",
    "def _winsorize_outliers(y: pd.Series, z=OUTLIER_Z):\n",
    "    if y.std() == 0:\n",
    "        return y\n",
    "    zscores = (y - y.mean()) / y.std()\n",
    "    clipped = y.copy()\n",
    "    mask_hi = zscores > z\n",
    "    mask_lo = zscores < -z\n",
    "    if mask_hi.any():\n",
    "        clipped[mask_hi] = y[~mask_hi].max()\n",
    "    if mask_lo.any():\n",
    "        clipped[mask_lo] = y[~mask_lo].min()\n",
    "    return clipped\n",
    "\n",
    "def _maybe_log_transform(s: pd.DataFrame) -> tuple[pd.DataFrame, bool]:\n",
    "    if not USE_LOG:\n",
    "        return s, False\n",
    "    if (s['y'] <= 0).any():\n",
    "        return s, False\n",
    "    y_range = s['y'].max() / max(s['y'].min(), LOG_TOL)\n",
    "    if y_range > 20:\n",
    "        s = s.copy()\n",
    "        s['y'] = np.log(s['y'] + LOG_TOL)\n",
    "        return s, True\n",
    "    return s, False\n",
    "\n",
    "def _inverse_log(yhat: np.ndarray) -> np.ndarray:\n",
    "    return np.exp(yhat) - LOG_TOL\n",
    "\n",
    "def _classify_temporal_resolution(train_df: pd.DataFrame) -> str:\n",
    "    if len(train_df) < 3:\n",
    "        return 'unknown'\n",
    "    deltas = np.diff(train_df['ds'].sort_values().values).astype('timedelta64[D]').astype(int)\n",
    "    med = np.median(deltas)\n",
    "    if med <= 2: return 'daily'\n",
    "    if med <= 10: return 'weekly'\n",
    "    if med <= 40: return 'monthly'\n",
    "    if med <= 120: return 'quarterly'\n",
    "    return 'yearly'\n",
    "\n",
    "# Iterate datasets\n",
    "for name, df in datasets.items():\n",
    "    if len(df) < fdw + holdout_window + forecast_window + 1:\n",
    "        continue\n",
    "\n",
    "    series = _prepare_series(df)\n",
    "    if len(series) <= holdout_window + MIN_TRAIN_POINTS:\n",
    "        continue\n",
    "\n",
    "    train_end = len(series) - holdout_window\n",
    "    train_df = series.iloc[:train_end].copy()\n",
    "    holdout_df = series.iloc[train_end:].copy()\n",
    "\n",
    "    inferred_freq = pd.infer_freq(train_df['ds'])\n",
    "    temporal_res = _classify_temporal_resolution(train_df)\n",
    "\n",
    "    local_enable_monthly = ENABLE_MONTHLY\n",
    "    local_enable_quarterly = ENABLE_QUARTERLY\n",
    "\n",
    "    if temporal_res == 'yearly':\n",
    "        freq = 'YS'\n",
    "        local_enable_monthly = False\n",
    "        local_enable_quarterly = False\n",
    "        effective_holdout = min(holdout_window, 10)\n",
    "    else:\n",
    "        freq = inferred_freq or 'MS'\n",
    "        effective_holdout = holdout_window\n",
    "\n",
    "    if ROBUST_TREND:\n",
    "        train_df = train_df.copy()\n",
    "        train_df['y'] = _winsorize_outliers(train_df['y'], z=OUTLIER_Z)\n",
    "\n",
    "    train_df, used_log = _maybe_log_transform(train_df)\n",
    "    if used_log:\n",
    "        holdout_df_t = holdout_df.copy()\n",
    "        holdout_df_t['y'] = np.log(holdout_df_t['y'] + LOG_TOL)\n",
    "    else:\n",
    "        holdout_df_t = holdout_df\n",
    "\n",
    "    if temporal_res == 'yearly':\n",
    "        yearly_orders = [5, 10]\n",
    "    else:\n",
    "        yearly_orders = HYPERPARAM_GRID['yearly_fourier']\n",
    "\n",
    "    cps_list = HYPERPARAM_GRID['changepoint_prior_scale']\n",
    "    sps_list = HYPERPARAM_GRID['seasonality_prior_scale']\n",
    "    cpr_list = HYPERPARAM_GRID['changepoint_range']\n",
    "    smode_list = HYPERPARAM_GRID['seasonality_mode']\n",
    "    growth_list = HYPERPARAM_GRID['growth']\n",
    "\n",
    "    best_mae = np.inf\n",
    "    best_cfg = None\n",
    "    best_pred = None\n",
    "\n",
    "    for cps, sps, yf, cpr, smode, growth in product(cps_list, sps_list, yearly_orders, cpr_list, smode_list, growth_list):\n",
    "        # Skip logistic if we applied log transform (incompatible semantics)\n",
    "        if used_log and growth == 'logistic':\n",
    "            continue\n",
    "\n",
    "        # Prepare working copies for logistic caps/floors\n",
    "        if growth == 'logistic':\n",
    "            # Ensure positive range; fallback to linear if degenerate\n",
    "            y_train_vals = train_df['y'].values\n",
    "            y_min, y_max = np.min(y_train_vals), np.max(y_train_vals)\n",
    "            if y_max <= y_min + 1e-9:\n",
    "                continue\n",
    "            cap_val = y_max * LOGISTIC_CAP_MULT\n",
    "            floor_val = y_min * LOGISTIC_FLOOR_MULT if y_min > 0 else 0.0\n",
    "            train_use = train_df.copy()\n",
    "            train_use['cap'] = cap_val\n",
    "            train_use['floor'] = floor_val\n",
    "        else:\n",
    "            train_use = train_df\n",
    "\n",
    "        try:\n",
    "            m = Prophet(\n",
    "                yearly_seasonality=False,\n",
    "                weekly_seasonality=False,\n",
    "                daily_seasonality=False,\n",
    "                changepoint_prior_scale=cps,\n",
    "                seasonality_prior_scale=sps,\n",
    "                changepoint_range=cpr,\n",
    "                seasonality_mode=smode,\n",
    "                growth=growth,\n",
    "            )\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Add seasonalities\n",
    "        try:\n",
    "            m.add_seasonality(name='yearly', period=365.25, fourier_order=yf)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if local_enable_monthly and temporal_res in {'monthly','weekly','daily'}:\n",
    "            m.add_seasonality(name='monthly', period=30.5, fourier_order=6)\n",
    "        if local_enable_quarterly and temporal_res in {'monthly','weekly','daily','quarterly'}:\n",
    "            m.add_seasonality(name='quarterly', period=91.25, fourier_order=4)\n",
    "        if temporal_res in {'daily','weekly'}:\n",
    "            # Provide moderate weekly component for higher frequency data\n",
    "            try:\n",
    "                m.add_seasonality(name='weekly_custom', period=7, fourier_order=4)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            m.fit(train_use)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            future = m.make_future_dataframe(periods=effective_holdout, freq=freq)\n",
    "        except OverflowError:\n",
    "            try:\n",
    "                future = m.make_future_dataframe(periods=min(3, effective_holdout), freq=freq)\n",
    "            except Exception:\n",
    "                continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Add cap/floor to future if logistic\n",
    "        if growth == 'logistic':\n",
    "            future['cap'] = cap_val\n",
    "            future['floor'] = floor_val\n",
    "\n",
    "        try:\n",
    "            fcst = m.predict(future).set_index('ds')\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        aligned_holdout = holdout_df.iloc[:effective_holdout]\n",
    "        needed = aligned_holdout['ds']\n",
    "        if not set(needed).issubset(fcst.index):\n",
    "            continue\n",
    "\n",
    "        yhat = fcst.loc[needed, 'yhat'].values\n",
    "        actual = aligned_holdout['y'].values\n",
    "        if used_log:\n",
    "            yhat = _inverse_log(yhat)\n",
    "            actual = aligned_holdout['y'].values  # already original scale\n",
    "\n",
    "        mae = mean_absolute_error(actual, yhat)\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_cfg = {\n",
    "                'changepoint_prior_scale': cps,\n",
    "                'seasonality_prior_scale': sps,\n",
    "                'yearly_fourier_order': yf,\n",
    "                'changepoint_range': cpr,\n",
    "                'seasonality_mode': smode,\n",
    "                'growth': growth,\n",
    "                'log_transform': used_log,\n",
    "                'freq': freq,\n",
    "                'temporal_resolution': temporal_res,\n",
    "                'effective_holdout': effective_holdout,\n",
    "            }\n",
    "            best_pred = yhat\n",
    "\n",
    "    if best_pred is None:\n",
    "        print(f\"{name}: Prophet failed; skipping.\")\n",
    "        continue\n",
    "\n",
    "    eval_holdout = holdout_df.iloc[:best_cfg['effective_holdout']]\n",
    "    mape = mean_absolute_percentage_error(eval_holdout['y'].values, best_pred)\n",
    "    prophet_maes[name] = best_mae\n",
    "    prophet_mapes[name] = mape\n",
    "    prophet_details[name] = {\n",
    "        'config': best_cfg,\n",
    "        'mae': best_mae,\n",
    "        'mape': mape,\n",
    "        'n_train': len(train_df),\n",
    "        'n_holdout_used': best_cfg['effective_holdout'],\n",
    "        'temporal_resolution': temporal_res,\n",
    "    }\n",
    "\n",
    "    print(f\"{name} (Prophet MAE): {best_mae:.3f}\")\n",
    "\n",
    "if prophet_maes:\n",
    "    avg_mae = sum(prophet_maes.values()) / len(prophet_maes)\n",
    "    avg_mape = sum(prophet_mapes.values()) / len(prophet_mapes)\n",
    "    print(f\"Average Prophet MAE: {avg_mae:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_performance = pd.DataFrame({'series':list(prophet_maes.keys()), 'prophet_mae':list(prophet_maes.values()), 'prophet_mape':list(prophet_mapes.values())})\n",
    "\n",
    "prophet_performance.to_csv(OUTPUT_DIR / 'prophet_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKtime Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified SKtime Performance (base sktime only) with full silent execution wrapper\n",
    "import logging, warnings, contextlib, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_error as sk_mae\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.compose import EnsembleForecaster, make_reduction\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    _HAS_XGB = True\n",
    "except Exception:\n",
    "    _HAS_XGB = False\n",
    "\n",
    "\n",
    "def _simple_split(df: pd.DataFrame, holdout: int):\n",
    "    df = df.sort_values('date')\n",
    "    y = pd.Series(df['value'].values, index=pd.to_datetime(df['date']))\n",
    "    if len(y) <= holdout + 5:\n",
    "        raise ValueError(\"Series too short\")\n",
    "    return y.iloc[:-holdout], y.iloc[-holdout:]\n",
    "\n",
    "\n",
    "def _make_es_grid():\n",
    "    \"\"\"Create a small, fast grid of ExponentialSmoothing models.\n",
    "\n",
    "    We keep this intentionally compact to avoid blowing up runtime.\n",
    "    Adjust `es_trends`, `es_seasonals`, `es_sp`, `damped_opts` to explore more.\n",
    "    \"\"\"\n",
    "    es_trends = ['add', None]              # try additive trend & no trend\n",
    "    es_seasonals = ['add', 'mul', None]    # additive, multiplicative, or none\n",
    "    es_sp = [12]                           # monthly-ish seasonality (adjust as needed)\n",
    "    damped_opts = [False, True]            # only meaningful if trend present\n",
    "\n",
    "    models = []\n",
    "    for tr in es_trends:\n",
    "        for seas in es_seasonals:\n",
    "            # If no seasonality, we don't actually need sp; but sktime still wants an int >=1\n",
    "            for sp in (es_sp if seas is not None else [1]):\n",
    "                for damp in damped_opts:\n",
    "                    if tr is None and damp:  # damped only valid with a trend component\n",
    "                        continue\n",
    "                    label = (\n",
    "                        f\"ETS-{tr or 'none'}-{seas or 'none'}\"\n",
    "                        f\"{'-damped' if damp else ''}-sp{sp if seas else 1}\"\n",
    "                    )\n",
    "                    try:\n",
    "                        mdl = ExponentialSmoothing(\n",
    "                            trend=tr,\n",
    "                            damped_trend=damp if tr is not None else False,\n",
    "                            seasonal=seas,\n",
    "                            sp=sp,\n",
    "                        )\n",
    "                        models.append((label, mdl))\n",
    "                    except Exception:\n",
    "                        # Skip any invalid combo silently\n",
    "                        continue\n",
    "    return models\n",
    "\n",
    "\n",
    "def run_sktime_benchmark_silent(datasets, holdout=24, ensemble_top=3, silent=True):\n",
    "    lines = []\n",
    "    result_mae = {}\n",
    "    best_model_map = {}\n",
    "    deseason_map = {}\n",
    "    detrend_map = {}\n",
    "    # Capture / suppress warnings + stdout/stderr\n",
    "    if silent:\n",
    "        outer_stdout = io.StringIO()\n",
    "        outer_stderr = io.StringIO()\n",
    "        warn_ctx = warnings.catch_warnings()\n",
    "        warn_ctx.__enter__()\n",
    "        warnings.simplefilter(\"ignore\")  # blanket ignore all warnings inside block\n",
    "        out_redirect = contextlib.redirect_stdout(outer_stdout)\n",
    "        err_redirect = contextlib.redirect_stderr(outer_stderr)\n",
    "        out_redirect.__enter__()\n",
    "        err_redirect.__enter__()\n",
    "    try:\n",
    "        for name, df in datasets.items():\n",
    "            if len(df) <= holdout + 25:\n",
    "                continue\n",
    "            try:\n",
    "                y_tr, y_ho = _simple_split(df, holdout)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if len(y_tr) < 30:\n",
    "                continue\n",
    "            fh = ForecastingHorizon(y_ho.index, is_relative=False)\n",
    "\n",
    "            # === Deseasonalize & Detrend (minimal addition) ===\n",
    "            y_tr_model = y_tr\n",
    "            deseason = None\n",
    "            detrender = None\n",
    "            deseason_reason = \"\"\n",
    "            detrend_reason = \"\"\n",
    "            try:\n",
    "                if len(y_tr) >= 12:\n",
    "                    try:\n",
    "                        deseason = Deseasonalizer()\n",
    "                        y_tr_model = deseason.fit_transform(y_tr_model)\n",
    "                        deseason_reason = \"applied\"\n",
    "                    except Exception:\n",
    "                        deseason = None\n",
    "                        deseason_reason = \"fail\"\n",
    "                else:\n",
    "                    deseason_reason = \"short\"\n",
    "                try:\n",
    "                    detrender = Detrender(forecaster=PolynomialTrendForecaster(degree=1))\n",
    "                    y_tr_model = detrender.fit_transform(y_tr_model)\n",
    "                    detrend_reason = \"applied\"\n",
    "                except Exception:\n",
    "                    detrender = None\n",
    "                    detrend_reason = \"fail\"\n",
    "            except Exception:\n",
    "                y_tr_model = y_tr\n",
    "                deseason = None\n",
    "                detrender = None\n",
    "                if not deseason_reason:\n",
    "                    deseason_reason = \"fail\"\n",
    "                if not detrend_reason:\n",
    "                    detrend_reason = \"fail\"\n",
    "\n",
    "            # Heuristic seasonal period guess for seasonal naive\n",
    "            sp_guess = 12 if len(y_tr) >= 24 else 1\n",
    "            try:\n",
    "                inf_freq = pd.infer_freq(y_tr.index)\n",
    "                if inf_freq:\n",
    "                    if inf_freq.startswith('W'):\n",
    "                        # weekly data\n",
    "                        sp_guess = 52 if len(y_tr) >= 104 else max(2, min(26, len(y_tr)//2))\n",
    "                    elif inf_freq[0] == 'M':\n",
    "                        sp_guess = 12\n",
    "                    elif inf_freq.startswith('Q'):\n",
    "                        sp_guess = 4\n",
    "                    elif inf_freq[0] == 'D':\n",
    "                        sp_guess = 7 if len(y_tr) >= 14 else 1\n",
    "                if sp_guess > len(y_tr)//2:\n",
    "                    sp_guess = max(1, len(y_tr)//4)\n",
    "            except Exception:\n",
    "                sp_guess = 12 if len(y_tr) >= 24 else 1\n",
    "            if sp_guess < 1:\n",
    "                sp_guess = 1\n",
    "\n",
    "            # Base (non-ES) models + Seasonal Naive\n",
    "            base_models = [\n",
    "                (\"Naive-last\", NaiveForecaster(strategy=\"last\")),\n",
    "                (\"Naive-drift\", NaiveForecaster(strategy=\"drift\")),\n",
    "                (\"Naive-seasonal\", NaiveForecaster(strategy=\"seasonal_last\", sp=sp_guess)),  # seasonal naive\n",
    "                (\"Theta\", ThetaForecaster(sp=12)),  # sp=12 seasonal period\n",
    "                (\"Trend1\", PolynomialTrendForecaster(degree=1)),\n",
    "                (\"Trend2\", PolynomialTrendForecaster(degree=2)),\n",
    "                (\"AutoARIMA\", AutoARIMA(suppress_warnings=True, maxiter=25)),\n",
    "                (\"AutoETS\", AutoETS(auto=True)),\n",
    "            ]\n",
    "\n",
    "            # Add ES hyperparameter grid models\n",
    "            es_models = _make_es_grid()\n",
    "\n",
    "            # Add Reduction models (tree/boost regressors)\n",
    "            reduction_models = []\n",
    "            try:\n",
    "                # Window lengths scale with series length but capped\n",
    "                rf_win = min(48, max(6, len(y_tr)//10))\n",
    "                reduction_models.append(\n",
    "                    (\"RF-Reduction\", make_reduction(\n",
    "                        RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1),\n",
    "                        strategy=\"recursive\",\n",
    "                        window_length=rf_win,\n",
    "                    ))\n",
    "                )\n",
    "                gb_win = min(36, max(6, len(y_tr)//12))\n",
    "                reduction_models.append(\n",
    "                    (\"GB-Reduction\", make_reduction(\n",
    "                        GradientBoostingRegressor(random_state=42),\n",
    "                        strategy=\"recursive\",\n",
    "                        window_length=gb_win,\n",
    "                    ))\n",
    "                )\n",
    "                if _HAS_XGB:\n",
    "                    xgb_win = min(48, max(6, len(y_tr)//10))\n",
    "                    reduction_models.append(\n",
    "                        (\"XGB-Reduction\", make_reduction(\n",
    "                            XGBRegressor(\n",
    "                                n_estimators=400,\n",
    "                                max_depth=4,\n",
    "                                learning_rate=0.05,\n",
    "                                subsample=0.9,\n",
    "                                colsample_bytree=0.8,\n",
    "                                objective='reg:squarederror',\n",
    "                                random_state=42,\n",
    "                                n_jobs=-1,\n",
    "                                verbosity=0,\n",
    "                            ),\n",
    "                            strategy=\"recursive\",\n",
    "                            window_length=xgb_win,\n",
    "                        ))\n",
    "                    )\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            models = base_models + es_models + reduction_models\n",
    "\n",
    "            scores = []\n",
    "            for lbl, mdl in models:\n",
    "                try:\n",
    "                    mdl.fit(y_tr_model)\n",
    "                    pred = mdl.predict(fh)\n",
    "                    # Inverse transforms (trend first, then season) if applied\n",
    "                    try:\n",
    "                        if detrender is not None:\n",
    "                            pred = detrender.inverse_transform(pred)\n",
    "                        if deseason is not None:\n",
    "                            pred = deseason.inverse_transform(pred)\n",
    "                    except Exception:\n",
    "                        pass  # if inversion fails, keep current pred\n",
    "                    mae = sk_mae(y_ho, pred)\n",
    "                    if np.isfinite(mae):\n",
    "                        scores.append((mae, lbl, mdl))\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if not scores:\n",
    "                continue\n",
    "            scores.sort(key=lambda x: x[0])\n",
    "            best_mae, best_lbl, _ = scores[0]\n",
    "\n",
    "            # Optional simple ensemble (ensemble operates on refit original transforms for consistency)\n",
    "            if ensemble_top and ensemble_top > 1 and len(scores) >= ensemble_top:\n",
    "                try:\n",
    "                    topk = scores[:ensemble_top]\n",
    "                    ens = EnsembleForecaster([(lbl, mdl) for _, lbl, mdl in topk])\n",
    "                    ens.fit(y_tr_model)\n",
    "                    e_pred = ens.predict(fh)\n",
    "                    try:\n",
    "                        if detrender is not None:\n",
    "                            e_pred = detrender.inverse_transform(e_pred)\n",
    "                        if deseason is not None:\n",
    "                            e_pred = deseason.inverse_transform(e_pred)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    e_mae = sk_mae(y_ho, e_pred)\n",
    "                    if e_mae < best_mae:\n",
    "                        best_mae = e_mae\n",
    "                        best_lbl = f\"EnsembleTop{ensemble_top}\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            result_mae[name] = float(best_mae)\n",
    "            best_model_map[name] = best_lbl\n",
    "            deseason_map[name] = deseason_reason or 'none'\n",
    "            detrend_map[name] = detrend_reason or 'none'\n",
    "            # Append transform status info\n",
    "            lines.append(\n",
    "                f\"{name}: {best_mae:.3f} (sktime {best_lbl}) [deseason={deseason_map[name]} detrend={detrend_map[name]}]\"\n",
    "            )\n",
    "    finally:\n",
    "        if silent:\n",
    "            # Exit contexts silently; discard captured text\n",
    "            err_redirect.__exit__(None, None, None)\n",
    "            out_redirect.__exit__(None, None, None)\n",
    "            warn_ctx.__exit__(None, None, None)\n",
    "\n",
    "    df_results = (\n",
    "        pd.DataFrame({\n",
    "            'dataset': list(result_mae.keys()),\n",
    "            'sktime_MAE': [result_mae[k] for k in result_mae.keys()],\n",
    "            'best_model': [best_model_map[k] for k in result_mae.keys()],\n",
    "            'deseason_status': [deseason_map[k] for k in result_mae.keys()],\n",
    "            'detrend_status': [detrend_map[k] for k in result_mae.keys()],\n",
    "        })\n",
    "        .sort_values('sktime_MAE').reset_index(drop=True)\n",
    "    )\n",
    "    return {'lines': lines, 'mae': result_mae, 'frame': df_results, 'best_model_map': best_model_map}\n",
    "\n",
    "# Run benchmark silently\n",
    "_sktime_out = run_sktime_benchmark_silent(datasets, holdout=24, ensemble_top=3, silent=True)\n",
    "sktime_results_df = _sktime_out['frame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sktime_performance = sktime_results_df[['dataset', 'sktime_MAE']].rename({'dataset':'series', 'sktime_MAE':'sktime_mae'}, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sktime_performance.to_csv(OUTPUT_DIR / 'sktime_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing AutoML, Prophet, and SKTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define series type mapping\n",
    "series_type_map = {\n",
    "    'Seasonal+Trend': 'Synthetic',\n",
    "    'Linear Trend': 'Synthetic',\n",
    "    'Quadratic': 'Synthetic',\n",
    "    'Logistic (S-curve)': 'Synthetic',\n",
    "    'Random Walk (drift)': 'Synthetic',\n",
    "    'Piecewise (changepoints)': 'Synthetic',\n",
    "    'Spiky Intermittent': 'Synthetic',\n",
    "    'Multi-seasonal': 'Synthetic',\n",
    "    'Sunspots': 'Real (statsmodels)',\n",
    "    'CO2': 'Real (statsmodels)',\n",
    "    'CPIAUCSL': 'Real (FRED)',\n",
    "    'UNRATE': 'Real (FRED)',\n",
    "    'INDPRO': 'Real (FRED)',\n",
    "}\n",
    "\n",
    "automl_performance = pd.read_csv(OUTPUT_DIR / 'automl_performance.csv')\n",
    "prophet_performance = pd.read_csv(OUTPUT_DIR / 'prophet_performance.csv')\n",
    "sktime_performance = pd.read_csv(OUTPUT_DIR / 'sktime_performance.csv')\n",
    "\n",
    "(automl_performance\n",
    " .merge(prophet_performance, on='series')\n",
    " .merge(sktime_performance, on='series')\n",
    " [['series', 'mae_challenger', 'prophet_mae', 'sktime_mae']]\n",
    " .rename({'mae_challenger': 'automl_mae'}, axis=1)\n",
    " .assign(series_type=lambda df_: df_['series'].map(series_type_map))\n",
    " .pipe(lambda df_: pd.concat([df_,\n",
    "                              (df_[['automl_mae', 'prophet_mae', 'sktime_mae']]\n",
    "                               .mean()\n",
    "                               .pipe(lambda df_: pd.DataFrame(df_).transpose())\n",
    "                               .assign(series=['Average'], series_type=[''])\n",
    "                               )]))\n",
    " .assign(winner=lambda df_: df_[[\"automl_mae\", \"prophet_mae\", \"sktime_mae\"]].idxmin(axis=1).str.replace(\"_mae\", \"\"))\n",
    " [['series', 'series_type', 'automl_mae', 'prophet_mae', 'sktime_mae', 'winner']]\n",
    " .to_csv(OUTPUT_DIR / 'combined_performance.csv', index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
