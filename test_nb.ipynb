{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest tests/integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl_tool.automl import AutoML\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='benign')\n",
    "\n",
    "# Split the dataset first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now add missing values directly to the training target\n",
    "np.random.seed(42)\n",
    "missing_indices = np.random.choice(y_train.index, size=10, replace=False)\n",
    "y_train.loc[missing_indices] = None\n",
    "\n",
    "# Initialize and fit the AutoML estimator\n",
    "automl = AutoML(X_train, y_train, \"benign\")\n",
    "automl.fit_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.fitted_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.get_feature_importance_scores()\n",
    "automl.plot_feature_importance_scores(top_k=15)\n",
    "automl.feature_importance_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO -- add feature: parameter to vary forecast window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl_tool.preprocessing import ts_train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "electric_tbl = pd.read_csv(\"/Users/andrewcarr/andrew_carr_website/posts/auto_ml/input_data/electric_production.csv\")\n",
    "\n",
    "plt.style.use(\"opinionated_rc\")\n",
    "plt.rcParams.update({'grid.linestyle': '-',})\n",
    "\n",
    "# Convert date column to datetime format \n",
    "electric_tbl['date'] = pd.to_datetime(electric_tbl['date'])\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.plot(electric_tbl['date'], electric_tbl['electricity_production'],\n",
    " label='Electricity Production', color='blue', linewidth = 1)\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Electricity Production', size = 9, loc = 'center')\n",
    "plt.title('Electricity Production Over Time', size = 16)\n",
    "\n",
    "# Set axis tick fontsize \n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dataframe and outcome variable (note - input dataframe includes outcome)\n",
    "X, y = electric_tbl, electric_tbl[\"electricity_production\"]\n",
    "\n",
    "# Outcome variable and date names \n",
    "outcome_var, date_var = \"electricity_production\", \"date\"\n",
    "\n",
    "# Feature derivation and holdout windows\n",
    "fdw, holdout_window = 18, 24\n",
    "\n",
    "X_train, X_holdout, y_train, y_holdout = ts_train_test_split(X, y, outcome_var, date_var, fdw, holdout_window, forecast_window=18)\n",
    "\n",
    "electric_automl_estimator = AutoML(X_train, y_train, \"electricity_production\", time_series=True)\n",
    "\n",
    "electric_automl_estimator.fit_pipeline(holdout_window=holdout_window)\n",
    "\n",
    "electric_automl_estimator.get_backtest_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple forecast windows: 1, 6, 12, 18\n",
    "forecast_windows = [1, 6, 12, 18]\n",
    "colors = ['red', 'orange', 'green', 'blue']\n",
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "for i, fw in enumerate(forecast_windows):\n",
    "    # print(f\"\\n--- Training model for forecast window: {fw} months ---\")\n",
    "    \n",
    "    # Split data with current forecast window\n",
    "    X_train_fw, X_holdout_fw, y_train_fw, y_holdout_fw = ts_train_test_split(\n",
    "        electric_tbl, electric_tbl[\"electricity_production\"], \n",
    "        \"electricity_production\", \"date\", \n",
    "        fdw=18, holdout_window=24, forecast_window=fw\n",
    "    )\n",
    "    \n",
    "    # Train AutoML model\n",
    "    automl_fw = AutoML(X_train_fw, y_train_fw, \"electricity_production\", time_series=True)\n",
    "    automl_fw.fit_pipeline(holdout_window=24)\n",
    "    \n",
    "    # Get predictions\n",
    "    preds_fw = automl_fw.fitted_pipeline.best_estimator_.predict(X_holdout_fw)\n",
    "    \n",
    "    # Store results\n",
    "    models[fw] = automl_fw\n",
    "    predictions[fw] = {\n",
    "        'X_holdout': X_holdout_fw,\n",
    "        'y_holdout': y_holdout_fw,\n",
    "        'predictions': preds_fw\n",
    "    }\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")\n",
    "\n",
    "# Create comparison plot\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "# Plot actuals (use forecast window 1 as reference since they should all have same holdout period)\n",
    "actual_values = predictions[1]['y_holdout'].to_numpy()\n",
    "actual_index = predictions[1]['X_holdout'].index\n",
    "\n",
    "ax.plot(actual_index, actual_values, label='Actual', color='black', linewidth=2)\n",
    "\n",
    "# Plot predictions for each forecast window\n",
    "for i, fw in enumerate(forecast_windows):\n",
    "    pred_data = predictions[fw]\n",
    "    ax.plot(pred_data['X_holdout'].index, pred_data['predictions'], \n",
    "            label=f'Predicted (FW={fw}m)', color=colors[i], \n",
    "            linestyle='--', alpha=0.8, linewidth=1.5)\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Electricity Production', size=12, loc='center')\n",
    "ax.set_title('Electricity Production: Forecast Window Comparison (Holdout Set)', size=18)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=12, loc='upper left', bbox_to_anchor=(0.02, 0.98))\n",
    "\n",
    "# Add caption explaining forecast windows\n",
    "fig.text(0.5, -0.05, \n",
    "         \"Note: FW=Forecast Window. FW=1m means predictions made 1 month ahead, FW=18m means predictions made 18 months ahead.\\nLonger forecast windows are more challenging and typically less accurate.\", \n",
    "         wrap=True, horizontalalignment='center', fontsize=10)\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, which='both', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "for fw in forecast_windows:\n",
    "    pred_data = predictions[fw]\n",
    "    mae = np.mean(np.abs(pred_data['predictions'] - pred_data['y_holdout'].to_numpy()))\n",
    "    print(f\"Forecast Window {fw}m - MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electric_automl_estimator.get_feature_importance_scores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electric_automl_estimator.plot_feature_importance_scores()\n",
    "\n",
    "electric_automl_estimator.feature_importance_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Evaluation (Multiple Datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install automl_tool in editable mode\n",
    "!pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from automl_tool.automl import AutoML\n",
    "# from automl_tool.preprocessing import ts_train_test_split\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Helpers\n",
    "make_dates = lambda n, freq='ME', start='2000-01-01': pd.date_range(start, periods=n, freq=freq)\n",
    "\n",
    "def df_from_values(values, start='2000-01-01', freq='ME'):\n",
    "    return pd.DataFrame({'date': make_dates(len(values), freq, start), 'value': np.asarray(values, dtype=float)})\n",
    "\n",
    "def ensure_regular_frequency(df: pd.DataFrame, date_col: str = 'date', value_col: str = 'value') -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d[date_col] = pd.to_datetime(d[date_col])\n",
    "    d = d.sort_values(date_col).drop_duplicates(subset=[date_col])\n",
    "    # Try to infer frequency; fallback to median delta if needed\n",
    "    freq = pd.infer_freq(d[date_col])\n",
    "    if freq is None:\n",
    "        deltas = d[date_col].diff().dropna()\n",
    "        if len(deltas) == 0:\n",
    "            freq = 'D'\n",
    "        else:\n",
    "            td = deltas.median()\n",
    "            try:\n",
    "                freq = to_offset(td)\n",
    "            except Exception:\n",
    "                freq = 'D'\n",
    "    full_index = pd.date_range(d[date_col].iloc[0], d[date_col].iloc[-1], freq=freq)\n",
    "    d = d.set_index(date_col).reindex(full_index)\n",
    "    # Interpolate missing values over time; fallback to ffill then bfill\n",
    "    if d[value_col].isna().any():\n",
    "        try:\n",
    "            d[value_col] = d[value_col].interpolate(method='time')\n",
    "        except Exception:\n",
    "            d[value_col] = d[value_col].ffill().bfill()\n",
    "    d = d.reset_index().rename(columns={'index': date_col})\n",
    "    return d[[date_col, value_col]]\n",
    "\n",
    "# Synthetic dataset generators (diverse shapes)\n",
    "def synth_seasonal(n=1000):\n",
    "    t = np.arange(n)\n",
    "    y = 10*np.sin(2*np.pi*t/12) + 0.5*t + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_linear(n=1000):\n",
    "    t = np.arange(n)\n",
    "    y = 0.3*t + np.random.normal(0, 3, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_quadratic(n=1000):\n",
    "    t = np.arange(n)\n",
    "    y = 0.01*(t-n/2)**2 + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_logistic(n=1000):\n",
    "    t = np.arange(n)\n",
    "    midpoint = n/2\n",
    "    y = 100/(1+np.exp(-(t-midpoint)/10)) + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_walk(n=1000):\n",
    "    rng = np.random.default_rng(123)\n",
    "    y = np.cumsum(rng.normal(0.3, 1.0, n))\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_piecewise(n=1000):\n",
    "    t = np.arange(n)\n",
    "    k1 = int(n*0.3); k2 = int(n*0.65)\n",
    "    base = np.piecewise(t,\n",
    "                        [t < k1, (t >= k1) & (t < k2), t >= k2],\n",
    "                        [lambda x: 0.2*x,\n",
    "                         lambda x: (0.2*k1) + (-0.1)*(x-k1),\n",
    "                         lambda x: (0.2*k1) + (-0.1)*(k2-k1) + 0.4*(x-k2)])\n",
    "    y = base + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_spiky(n=1000):\n",
    "    rng = np.random.default_rng(0)\n",
    "    y = 20 + np.sin(2*np.pi*np.arange(n)/24) + rng.normal(0, 2, n)\n",
    "    idx = rng.choice(n, size=max(10, n//30), replace=False)\n",
    "    y[idx] += rng.uniform(10, 25, len(idx))\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_multiseason(n=1000):\n",
    "    t = np.arange(n)\n",
    "    y = 5*np.sin(2*np.pi*t/12) + 3*np.sin(2*np.pi*t/6) + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "# Collect datasets (n=1000 each synthetic)\n",
    "datasets = {\n",
    "    'Seasonal+Trend': synth_seasonal(n=1000),\n",
    "    'Linear Trend': synth_linear(n=1000),\n",
    "    'Quadratic': synth_quadratic(n=1000),\n",
    "    'Logistic (S-curve)': synth_logistic(n=1000),\n",
    "    'Random Walk (drift)': synth_walk(n=1000),\n",
    "    'Piecewise (changepoints)': synth_piecewise(n=1000),\n",
    "    'Spiky Intermittent': synth_spiky(n=1000),\n",
    "    'Multi-seasonal': synth_multiseason(n=1000),\n",
    "}\n",
    "\n",
    "# Add real datasets from statsmodels\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    # Sunspots (yearly)\n",
    "    try:\n",
    "        sun = sm.datasets.sunspots.load_pandas().data\n",
    "        df_sun = pd.DataFrame({\n",
    "            'date': pd.to_datetime(sun['YEAR'], format='%Y', errors='coerce'),\n",
    "            'value': sun['SUNACTIVITY'].astype(float)\n",
    "        }).dropna()\n",
    "        datasets['Sunspots'] = df_sun\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Mauna Loa CO2 (weekly)\n",
    "    try:\n",
    "        co2 = sm.datasets.co2.load_pandas().data\n",
    "        co2 = co2.copy()\n",
    "        if 'co2' in co2.columns and 'date' not in co2.columns:\n",
    "            co2 = co2.reset_index().rename(columns={'index': 'date', 'co2': 'value'})\n",
    "        else:\n",
    "            if 'date' not in co2.columns:\n",
    "                co2 = co2.reset_index().rename(columns={'index': 'date'})\n",
    "            if 'value' not in co2.columns:\n",
    "                first_val = [c for c in co2.columns if c != 'date'][0]\n",
    "                co2 = co2.rename(columns={first_val: 'value'})\n",
    "        co2 = co2[['date', 'value']].dropna()\n",
    "        datasets['CO2'] = co2\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Add selected FRED series\n",
    "try:\n",
    "    from pandas_datareader import data as pdr\n",
    "    fred_codes = ['CPIAUCSL', 'UNRATE', 'INDPRO']\n",
    "    for code in fred_codes:\n",
    "        try:\n",
    "            df_fred = pdr.DataReader(code, 'fred', start='1990-01-01')\n",
    "            df_fred = df_fred.rename(columns={code: 'value'}).reset_index().rename(columns={'DATE': 'date'})\n",
    "            df_fred = df_fred.dropna()\n",
    "            datasets[code] = df_fred\n",
    "        except Exception:\n",
    "            continue\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Filter to only the datasets the user wants\n",
    "allowed = {\n",
    "    'Seasonal+Trend', 'Linear Trend', 'Quadratic', 'Logistic (S-curve)',\n",
    "    'Random Walk (drift)', 'Piecewise (changepoints)', 'Spiky Intermittent',\n",
    "    'Multi-seasonal', 'Sunspots', 'CO2', 'CPIAUCSL', 'UNRATE', 'INDPRO'\n",
    "}\n",
    "datasets = {k: v for k, v in datasets.items() if k in allowed}\n",
    "\n",
    "winners = []\n",
    "# Normalize to equidistant dates\n",
    "for k in list(datasets.keys()):\n",
    "    datasets[k] = ensure_regular_frequency(datasets[k], 'date', 'value')\n",
    "\n",
    "fdw, holdout_window, forecast_window = 18, 24, 1\n",
    "maes = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if len(df) < fdw + holdout_window + forecast_window + 1:\n",
    "        continue\n",
    "    X, y = df, df['value']\n",
    "    X_train, X_holdout, y_train, y_holdout = ts_train_test_split(\n",
    "        X, y, 'value', 'date', fdw, holdout_window, forecast_window=forecast_window\n",
    ")\n",
    "    # Skip datasets that are too short for 5-fold TimeSeriesSplit with this test size\n",
    "    if len(X_train) <= 5 * holdout_window:\n",
    "        continue\n",
    "    automl_mod = AutoML(X_train, y_train, 'value', time_series=True)\n",
    "    automl_mod.fit_pipeline(holdout_window=holdout_window)\n",
    "    preds = automl_mod.fitted_pipeline.best_estimator_.predict(X_holdout)\n",
    "    mae = mean_absolute_error(y_holdout, preds)\n",
    "    maes.append(mae)\n",
    "    print(f\"{name}: {mae:.3f}\")\n",
    "\t# ... inside your datasets loop, after fit:\n",
    "    winners.append(type(automl_mod.fitted_pipeline.best_estimator_.get_params()['model']).__name__)\n",
    "\n",
    "if maes:\n",
    "    print(f\"Average MAE: {float(np.mean(maes)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB forecasting block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def ts_train_test_split(\n",
    "    X: pd.DataFrame, \n",
    "    y: pd.Series, \n",
    "    outcome_col: str, \n",
    "    date_col: str, \n",
    "    fdw: int, \n",
    "    holdout_window: int,\n",
    "    forecast_window: Optional[int] = 1 \n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Apply preprocessing and split the data into training and testing sets for time series modeling.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper function to preprocess ts data\n",
    "    def _ts_preproc(inp_tbl: pd.DataFrame, inp_y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:   \n",
    "        preproc_tbl = (inp_tbl\n",
    "        .pipe(lambda x: x.assign(**{f\"lagged_{outcome_col}_{i}m\": x[outcome_col].shift(i) for i in range(forecast_window, fdw + 1)}))\n",
    "        .pipe(lambda x: x.assign(**{f\"inv_hyp_sin_lagged_{outcome_col}_{i}m\": np.arcsinh(x[outcome_col].shift(i)) for i in range(forecast_window, fdw + 1)}))\n",
    "        .pipe(lambda x: x.assign(**{f\"rolling_avg_{outcome_col}_{i}m\": x[outcome_col].shift(1).rolling(window=i).mean() for i in range(forecast_window, fdw + 1)}))\n",
    "        .pipe(lambda x: x.assign(**{f\"min_{outcome_col}_{i}m\": x[outcome_col].shift(1).rolling(window=i).min() for i in range(forecast_window, fdw + 1)}))\n",
    "        # New time and seasonal features\n",
    "        .pipe(lambda x: x.assign(\n",
    "            # t=np.arange(len(x)),\n",
    "            monthsin=np.sin(2 * np.pi * pd.to_datetime(x[date_col]).dt.month / 12.0),\n",
    "            monthcos=np.cos(2 * np.pi * pd.to_datetime(x[date_col]).dt.month / 12.0),\n",
    "        ))\n",
    "        # Drop the original date and outcome columns\n",
    "        .drop([date_col, outcome_col], axis=1)\n",
    "        # Rowwise deletion of missing values\n",
    "        .dropna(axis=0)\n",
    "        )\n",
    "        preproc_y = inp_y.loc[preproc_tbl.index]\n",
    "\n",
    "        return preproc_tbl, preproc_y\n",
    "\n",
    "    # Reset index of X and y\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Calculate the index to split the data\n",
    "    train_end_index = X.shape[0] - (holdout_window)\n",
    "    test_start_index = X.shape[0] - (fdw + holdout_window)\n",
    "\n",
    "    # Split the data\n",
    "    X_train = X.iloc[:train_end_index]\n",
    "    X_test = X.iloc[test_start_index:]\n",
    "    y_train = y.iloc[:train_end_index]\n",
    "    y_test = y.iloc[test_start_index:]\n",
    "\n",
    "    # Set the indices of both X and y train/test to the 'date' column \n",
    "    X_train.set_index(date_col, drop=False, inplace=True)\n",
    "    y_train.index = X_train.index\n",
    "    X_test.set_index(date_col, drop=False, inplace=True)\n",
    "    y_test.index = X_test.index\n",
    "\n",
    "    # Preprocess the data\n",
    "    X_train, y_train = _ts_preproc(X_train, y_train)\n",
    "    X_test, y_test = _ts_preproc(X_test, y_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from automl_tool.estimation import XGBWithEarlyStoppingRegressor\n",
    "from pandas_datareader import data as pdr\n",
    "from automl_tool.preprocessing import ts_train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np \n",
    "\n",
    "fdw, holdout_window, forecast_window = 12, 24, 1\n",
    "\n",
    "df_fred = pdr.DataReader(\"CPIAUCSL\", 'fred', start='1990-01-01')\n",
    "df_fred = df_fred.rename(columns={'CPIAUCSL': 'value'}).reset_index().rename(columns={'DATE': 'date'})\n",
    "df_fred = df_fred.dropna()\n",
    "\n",
    "X, y = df_fred, df_fred['value']\n",
    "X_train, X_holdout, y_train, y_holdout = ts_train_test_split(\n",
    "\tX, y, 'value', 'date', fdw, holdout_window, forecast_window=forecast_window\n",
    ")\n",
    "\n",
    "xgb_model = XGBWithEarlyStoppingRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "preds = xgb_model.predict(X_holdout)\n",
    "mae = mean_absolute_error(y_holdout, preds)\n",
    "print(f\"XGBRegressor MAE on CPIAUCSL: {mae:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoML forecasting block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl_tool.automl import AutoML\n",
    "\n",
    "automl_mod = AutoML(X_train, y_train, 'value', time_series=True)\n",
    "\n",
    "automl_mod.fit_pipeline(holdout_window=holdout_window)\n",
    "\n",
    "# Get the best model from the fitted pipeline\n",
    "y_preds = automl_mod.fitted_pipeline.predict(X_holdout)\n",
    "\n",
    "mean_absolute_error(y_holdout, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot actual and predicts on holdout set \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_holdout.index, y_holdout, label='Actual', color='blue')\n",
    "plt.plot(y_holdout.index, y_preds, label='Predicted', color='orange')\n",
    "legend = plt.legend(loc='upper left', fontsize=12)\n",
    "plt.title('Predictions vs Actuals on Holdout Set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_mod.fitted_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalecast forecasting block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from scalecast.Forecaster import Forecaster\n",
    "from scalecast import GridGenerator\n",
    "\n",
    "GridGenerator.get_example_grids()  # example hyperparameter grids\n",
    "\n",
    "data = df_fred\n",
    "f = Forecaster(\n",
    "    y=data['value'],               # required\n",
    "    current_dates=data['date'],    # required\n",
    "    future_dates=1,               # length of the forecast horizon\n",
    "    test_length=24,                 # set a test set length or fraction to validate all models if desired\n",
    "    cis=False,                     # choose whether or not to evaluate confidence intervals for all models\n",
    ")\n",
    "f.set_estimator('xgboost')  # select an estimator\n",
    "\n",
    "f.auto_Xvar_select()       # find best look-back, trend, and seasonality for your series\n",
    "f.cross_validate(k=3)       # tune model hyperparams using time series cross validation\n",
    "f.auto_forecast()           # automatically forecast with the chosen Xvars and hyperparams\n",
    "\n",
    "results = f.export(['lvl_fcsts','model_summaries'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_preds = f.export('lvl_test_set_predictions')\n",
    "\n",
    "mean_absolute_error(ts_preds['actual'], ts_preds['xgboost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_holdout.index, ts_preds['actual'], label='Actual', color='blue')\n",
    "plt.plot(y_holdout.index, ts_preds['xgboost'], label='Predicted', color='orange')\n",
    "legend = plt.legend(loc='upper left', fontsize=12)\n",
    "plt.title('XGBWithEarlyStoppingRegressor Predictions vs Actuals on Holdout Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scalecast import GridGenerator\n",
    "GridGenerator.get_example_grids()   # writes Grids.py to your working dir (if not already present)\n",
    "\n",
    "# then either open Grids.py in your editor, or import it:\n",
    "from Grids import xgboost as xgb_grid\n",
    "print(xgb_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['model_summaries']['HyperParams'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
