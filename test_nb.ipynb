{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Evaluation (Multiple Datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/430016232/Library/CloudStorage/OneDrive-EnactMortgageInsuranceCompany/Documents/research/automl_tool\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from automl_tool==0.1.0) (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.18 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from automl_tool==0.1.0) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn<=1.5.1,>=0.24 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from automl_tool==0.1.0) (1.5.1)\n",
      "Requirement already satisfied: shap>=0.39 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from automl_tool==0.1.0) (0.48.0)\n",
      "Requirement already satisfied: seaborn>=0.11 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from automl_tool==0.1.0) (0.13.2)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from automl_tool==0.1.0) (3.10.6)\n",
      "Requirement already satisfied: xgboost>=1.3 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from automl_tool==0.1.0) (3.0.5)\n",
      "Requirement already satisfied: opinionated in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from automl_tool==0.1.0) (0.0.3.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from automl_tool==0.1.0) (8.1.7)\n",
      "Requirement already satisfied: pytest>=8.3.4 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from automl_tool==0.1.0) (8.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from scikit-learn<=1.5.1,>=0.24->automl_tool==0.1.0) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from scikit-learn<=1.5.1,>=0.24->automl_tool==0.1.0) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from scikit-learn<=1.5.1,>=0.24->automl_tool==0.1.0) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from matplotlib>=3.3->automl_tool==0.1.0) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from matplotlib>=3.3->automl_tool==0.1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from matplotlib>=3.3->automl_tool==0.1.0) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from matplotlib>=3.3->automl_tool==0.1.0) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from matplotlib>=3.3->automl_tool==0.1.0) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from matplotlib>=3.3->automl_tool==0.1.0) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from matplotlib>=3.3->automl_tool==0.1.0) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from matplotlib>=3.3->automl_tool==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from pandas>=1.0->automl_tool==0.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from pandas>=1.0->automl_tool==0.1.0) (2025.2)\n",
      "Requirement already satisfied: iniconfig>=1 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from pytest>=8.3.4->automl_tool==0.1.0) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from pytest>=8.3.4->automl_tool==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from pytest>=8.3.4->automl_tool==0.1.0) (2.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3->automl_tool==0.1.0) (1.17.0)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from shap>=0.39->automl_tool==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from shap>=0.39->automl_tool==0.1.0) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from shap>=0.39->automl_tool==0.1.0) (0.62.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from shap>=0.39->automl_tool==0.1.0) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from shap>=0.39->automl_tool==0.1.0) (4.15.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from numba>=0.54->shap>=0.39->automl_tool==0.1.0) (0.45.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from ipywidgets->automl_tool==0.1.0) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from ipywidgets->automl_tool==0.1.0) (9.5.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from ipywidgets->automl_tool==0.1.0) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from ipywidgets->automl_tool==0.1.0) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from ipywidgets->automl_tool==0.1.0) (3.0.15)\n",
      "Requirement already satisfied: decorator in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (3.0.52)\n",
      "Requirement already satisfied: stack_data in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: requests in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from opinionated->automl_tool==0.1.0) (2.32.5)\n",
      "Requirement already satisfied: colormaps in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from opinionated->automl_tool==0.1.0) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from requests->opinionated->automl_tool==0.1.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from requests->opinionated->automl_tool==0.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from requests->opinionated->automl_tool==0.1.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from requests->opinionated->automl_tool==0.1.0) (2025.8.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /Users/430016232/miniconda3/envs/automl_env/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->automl_tool==0.1.0) (0.2.3)\n",
      "Building wheels for collected packages: automl_tool\n",
      "  Building editable for automl_tool (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for automl_tool: filename=automl_tool-0.1.0-0.editable-py3-none-any.whl size=2921 sha256=1488d7a54dad47129b8f17819f6a069b74c871dd4974af16434c27f2039b8d75\n",
      "  Stored in directory: /private/var/folders/sz/43lqpbtj4n3dyzfj6zq2ljgw0000gq/T/pip-ephem-wheel-cache-4jh7y36o/wheels/c6/37/80/5eb8f2f50997eaa77b7c4671e4ff5decdee87e31c513e5fe4a\n",
      "Successfully built automl_tool\n",
      "Installing collected packages: automl_tool\n",
      "  Attempting uninstall: automl_tool\n",
      "    Found existing installation: automl_tool 0.1.0\n",
      "    Uninstalling automl_tool-0.1.0:\n",
      "      Successfully uninstalled automl_tool-0.1.0\n",
      "Successfully installed automl_tool-0.1.0\n"
     ]
    }
   ],
   "source": [
    "# Install automl_tool in editable mode\n",
    "!pip install -e .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from automl_tool.automl import AutoML\n",
    "from automl_tool.preprocessing import ts_train_test_split\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Helpers\n",
    "make_dates = lambda n, freq='ME', start='2000-01-01': pd.date_range(start, periods=n, freq=freq)\n",
    "\n",
    "def df_from_values(values, start='2000-01-01', freq='ME'):\n",
    "    return pd.DataFrame({'date': make_dates(len(values), freq, start), 'value': np.asarray(values, dtype=float)})\n",
    "\n",
    "def ensure_regular_frequency(df: pd.DataFrame, date_col: str = 'date', value_col: str = 'value') -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d[date_col] = pd.to_datetime(d[date_col])\n",
    "    d = d.sort_values(date_col).drop_duplicates(subset=[date_col])\n",
    "    # Try to infer frequency; fallback to median delta if needed\n",
    "    freq = pd.infer_freq(d[date_col])\n",
    "    if freq is None:\n",
    "        deltas = d[date_col].diff().dropna()\n",
    "        if len(deltas) == 0:\n",
    "            freq = 'D'\n",
    "        else:\n",
    "            td = deltas.median()\n",
    "            try:\n",
    "                freq = to_offset(td)\n",
    "            except Exception:\n",
    "                freq = 'D'\n",
    "    full_index = pd.date_range(d[date_col].iloc[0], d[date_col].iloc[-1], freq=freq)\n",
    "    d = d.set_index(date_col).reindex(full_index)\n",
    "    # Interpolate missing values over time; fallback to ffill then bfill\n",
    "    if d[value_col].isna().any():\n",
    "        try:\n",
    "            d[value_col] = d[value_col].interpolate(method='time')\n",
    "        except Exception:\n",
    "            d[value_col] = d[value_col].ffill().bfill()\n",
    "    d = d.reset_index().rename(columns={'index': date_col})\n",
    "    return d[[date_col, value_col]]\n",
    "\n",
    "# Synthetic dataset generators (diverse shapes)\n",
    "def synth_seasonal(n=1000):\n",
    "    t = np.arange(n)\n",
    "    y = 10*np.sin(2*np.pi*t/12) + 0.5*t + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_linear(n=1000):\n",
    "    t = np.arange(n)\n",
    "    y = 0.3*t + np.random.normal(0, 3, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_quadratic(n=1000):\n",
    "    t = np.arange(n)\n",
    "    y = 0.01*(t-n/2)**2 + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_logistic(n=1000):\n",
    "    t = np.arange(n)\n",
    "    midpoint = n/2\n",
    "    y = 100/(1+np.exp(-(t-midpoint)/10)) + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_walk(n=1000):\n",
    "    rng = np.random.default_rng(123)\n",
    "    y = np.cumsum(rng.normal(0.3, 1.0, n))\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_piecewise(n=1000):\n",
    "    t = np.arange(n)\n",
    "    k1 = int(n*0.3); k2 = int(n*0.65)\n",
    "    base = np.piecewise(t,\n",
    "                        [t < k1, (t >= k1) & (t < k2), t >= k2],\n",
    "                        [lambda x: 0.2*x,\n",
    "                         lambda x: (0.2*k1) + (-0.1)*(x-k1),\n",
    "                         lambda x: (0.2*k1) + (-0.1)*(k2-k1) + 0.4*(x-k2)])\n",
    "    y = base + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_spiky(n=1000):\n",
    "    rng = np.random.default_rng(0)\n",
    "    y = 20 + np.sin(2*np.pi*np.arange(n)/24) + rng.normal(0, 2, n)\n",
    "    idx = rng.choice(n, size=max(10, n//30), replace=False)\n",
    "    y[idx] += rng.uniform(10, 25, len(idx))\n",
    "    return df_from_values(y)\n",
    "\n",
    "def synth_multiseason(n=1000):\n",
    "    t = np.arange(n)\n",
    "    y = 5*np.sin(2*np.pi*t/12) + 3*np.sin(2*np.pi*t/6) + np.random.normal(0, 2, n)\n",
    "    return df_from_values(y)\n",
    "\n",
    "# Collect datasets (n=1000 each synthetic)\n",
    "datasets = {\n",
    "    'Seasonal+Trend': synth_seasonal(n=1000),\n",
    "    'Linear Trend': synth_linear(n=1000),\n",
    "    'Quadratic': synth_quadratic(n=1000),\n",
    "    'Logistic (S-curve)': synth_logistic(n=1000),\n",
    "    'Random Walk (drift)': synth_walk(n=1000),\n",
    "    'Piecewise (changepoints)': synth_piecewise(n=1000),\n",
    "    'Spiky Intermittent': synth_spiky(n=1000),\n",
    "    'Multi-seasonal': synth_multiseason(n=1000),\n",
    "}\n",
    "\n",
    "# Add real datasets from statsmodels\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    # Sunspots (yearly)\n",
    "    try:\n",
    "        sun = sm.datasets.sunspots.load_pandas().data\n",
    "        df_sun = pd.DataFrame({\n",
    "            'date': pd.to_datetime(sun['YEAR'], format='%Y', errors='coerce'),\n",
    "            'value': sun['SUNACTIVITY'].astype(float)\n",
    "        }).dropna()\n",
    "        datasets['Sunspots'] = df_sun\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Mauna Loa CO2 (weekly)\n",
    "    try:\n",
    "        co2 = sm.datasets.co2.load_pandas().data\n",
    "        co2 = co2.copy()\n",
    "        if 'co2' in co2.columns and 'date' not in co2.columns:\n",
    "            co2 = co2.reset_index().rename(columns={'index': 'date', 'co2': 'value'})\n",
    "        else:\n",
    "            if 'date' not in co2.columns:\n",
    "                co2 = co2.reset_index().rename(columns={'index': 'date'})\n",
    "            if 'value' not in co2.columns:\n",
    "                first_val = [c for c in co2.columns if c != 'date'][0]\n",
    "                co2 = co2.rename(columns={first_val: 'value'})\n",
    "        co2 = co2[['date', 'value']].dropna()\n",
    "        datasets['CO2'] = co2\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Add selected FRED series\n",
    "try:\n",
    "    from pandas_datareader import data as pdr\n",
    "    fred_codes = ['CPIAUCSL', 'UNRATE', 'INDPRO']\n",
    "    for code in fred_codes:\n",
    "        try:\n",
    "            df_fred = pdr.DataReader(code, 'fred', start='1990-01-01')\n",
    "            df_fred = df_fred.rename(columns={code: 'value'}).reset_index().rename(columns={'DATE': 'date'})\n",
    "            df_fred = df_fred.dropna()\n",
    "            datasets[code] = df_fred\n",
    "        except Exception:\n",
    "            continue\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Filter to only the datasets the user wants\n",
    "allowed = {\n",
    "    'Seasonal+Trend', 'Linear Trend', 'Quadratic', 'Logistic (S-curve)',\n",
    "    'Random Walk (drift)', 'Piecewise (changepoints)', 'Spiky Intermittent',\n",
    "    'Multi-seasonal', 'Sunspots', 'CO2', 'CPIAUCSL', 'UNRATE', 'INDPRO'\n",
    "}\n",
    "datasets = {k: v for k, v in datasets.items() if k in allowed}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seasonal+Trend: 1.861\n",
      "Linear Trend: 2.304\n",
      "Quadratic: 1.872\n",
      "Logistic (S-curve): 2.419\n",
      "Random Walk (drift): 0.708\n",
      "Piecewise (changepoints): 2.421\n",
      "Spiky Intermittent: 1.739\n",
      "Multi-seasonal: 1.481\n",
      "Sunspots: 9.909\n",
      "CO2: 0.216\n",
      "CPIAUCSL: 0.402\n",
      "UNRATE: 0.112\n",
      "INDPRO: 0.484\n",
      "Average MAE: 1.994\n"
     ]
    }
   ],
   "source": [
    "\n",
    "winners = []\n",
    "# Normalize to equidistant dates\n",
    "for k in list(datasets.keys()):\n",
    "    datasets[k] = ensure_regular_frequency(datasets[k], 'date', 'value')\n",
    "\n",
    "fdw, holdout_window, forecast_window = 18, 24, 1\n",
    "maes = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if len(df) < fdw + holdout_window + forecast_window + 1:\n",
    "        continue\n",
    "    X, y = df, df['value']\n",
    "    X_train, X_holdout, y_train, y_holdout = ts_train_test_split(\n",
    "        X, y, 'value', 'date', fdw, holdout_window, forecast_window=forecast_window\n",
    ")\n",
    "    # Skip datasets that are too short for 5-fold TimeSeriesSplit with this test size\n",
    "    if len(X_train) <= 5 * holdout_window:\n",
    "        continue\n",
    "    automl_mod = AutoML(X_train, y_train, 'value', time_series=True)\n",
    "    automl_mod.fit_pipeline(holdout_window=holdout_window)\n",
    "    preds = automl_mod.fitted_pipeline.best_estimator_.predict(X_holdout)\n",
    "    mae = mean_absolute_error(y_holdout, preds)\n",
    "    maes.append(mae)\n",
    "    print(f\"{name}: {mae:.3f}\")\n",
    "\t# ... inside your datasets loop, after fit:\n",
    "    winners.append(type(automl_mod.fitted_pipeline.best_estimator_.get_params()['model']).__name__)\n",
    "\n",
    "if maes:\n",
    "    print(f\"Average MAE: {float(np.mean(maes)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3344166666666666"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((1.994 * 13) - 9.909)/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((2.493 * 10) - 9.909)/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ElasticNet',\n",
       " 'ElasticNet',\n",
       " 'ElasticNet',\n",
       " 'ElasticNet',\n",
       " 'ElasticNet',\n",
       " 'ElasticNet',\n",
       " 'ElasticNet',\n",
       " 'XGBWithEarlyStoppingRegressor',\n",
       " 'ElasticNet',\n",
       " 'ElasticNet',\n",
       " 'ElasticNet',\n",
       " 'ElasticNet',\n",
       " 'ElasticNet']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seasonal+Trend (Prophet MAE): 1.907\n",
      "Linear Trend (Prophet MAE): 2.228\n",
      "Linear Trend (Prophet MAE): 2.228\n",
      "Quadratic (Prophet MAE): 12.980\n",
      "Quadratic (Prophet MAE): 12.980\n",
      "Logistic (S-curve) (Prophet MAE): 2.874\n",
      "Logistic (S-curve) (Prophet MAE): 2.874\n",
      "Random Walk (drift) (Prophet MAE): 1.662\n",
      "Random Walk (drift) (Prophet MAE): 1.662\n",
      "Piecewise (changepoints) (Prophet MAE): 1.911\n",
      "Piecewise (changepoints) (Prophet MAE): 1.911\n",
      "Spiky Intermittent (Prophet MAE): 1.648\n",
      "Spiky Intermittent (Prophet MAE): 1.648\n",
      "Multi-seasonal (Prophet MAE): 1.404\n",
      "Multi-seasonal (Prophet MAE): 1.404\n",
      "Sunspots: Prophet failed; skipping.\n",
      "Sunspots: Prophet failed; skipping.\n",
      "CO2 (Prophet MAE): 0.365\n",
      "CO2 (Prophet MAE): 0.365\n",
      "CPIAUCSL (Prophet MAE): 1.494\n",
      "CPIAUCSL (Prophet MAE): 1.494\n",
      "UNRATE (Prophet MAE): 0.256\n",
      "UNRATE (Prophet MAE): 0.256\n",
      "INDPRO (Prophet MAE): 0.868\n",
      "Average Prophet MAE: 2.466\n",
      "INDPRO (Prophet MAE): 0.868\n",
      "Average Prophet MAE: 2.466\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('cmdstanpy')\n",
    "logger.handlers = []\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from itertools import product\n",
    "\n",
    "# Enhanced Prophet benchmarking block (with yearly-series safeguards + expanded hyperparameter grid)\n",
    "# Configuration\n",
    "MIN_TRAIN_POINTS = 30               # skip very short series\n",
    "LOG_TOL = 1e-6                      # small constant for log transform\n",
    "OUTLIER_Z = 4.0                     # z-score threshold for winsorization\n",
    "HYPERPARAM_GRID = {\n",
    "    'changepoint_prior_scale': [.005, 0.05, 0.5],\n",
    "    'seasonality_prior_scale': [5.0, 15.0],\n",
    "    'yearly_fourier': [10, 20],\n",
    "    'changepoint_range': [0.8, 0.9],\n",
    "    'seasonality_mode': ['additive', 'multiplicative'],\n",
    "    'growth': ['linear', 'logistic']  # logistic will auto-add cap/floor\n",
    "}\n",
    "ENABLE_MONTHLY = True\n",
    "ENABLE_QUARTERLY = True\n",
    "USE_LOG = True                      # per-series decision still checked\n",
    "ROBUST_TREND = True                 # remove extreme spikes before fit\n",
    "LOGISTIC_CAP_MULT = 1.15            # cap = max(train_y) * this\n",
    "LOGISTIC_FLOOR_MULT = 0.85          # floor = min(train_y) * this (if positive)\n",
    "\n",
    "prophet_maes = {}\n",
    "prophet_mapes = {}\n",
    "prophet_details = {}\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def _prepare_series(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = (df[['date','value']]\n",
    "          .rename(columns={'date':'ds','value':'y'})\n",
    "          .sort_values('ds')\n",
    "          .reset_index(drop=True))\n",
    "    s = s.drop_duplicates(subset='ds')\n",
    "    s['ds'] = pd.to_datetime(s['ds'])\n",
    "    s = s.dropna(subset=['y'])\n",
    "    return s\n",
    "\n",
    "def _winsorize_outliers(y: pd.Series, z=OUTLIER_Z):\n",
    "    if y.std() == 0:\n",
    "        return y\n",
    "    zscores = (y - y.mean()) / y.std()\n",
    "    clipped = y.copy()\n",
    "    mask_hi = zscores > z\n",
    "    mask_lo = zscores < -z\n",
    "    if mask_hi.any():\n",
    "        clipped[mask_hi] = y[~mask_hi].max()\n",
    "    if mask_lo.any():\n",
    "        clipped[mask_lo] = y[~mask_lo].min()\n",
    "    return clipped\n",
    "\n",
    "def _maybe_log_transform(s: pd.DataFrame) -> tuple[pd.DataFrame, bool]:\n",
    "    if not USE_LOG:\n",
    "        return s, False\n",
    "    if (s['y'] <= 0).any():\n",
    "        return s, False\n",
    "    y_range = s['y'].max() / max(s['y'].min(), LOG_TOL)\n",
    "    if y_range > 20:\n",
    "        s = s.copy()\n",
    "        s['y'] = np.log(s['y'] + LOG_TOL)\n",
    "        return s, True\n",
    "    return s, False\n",
    "\n",
    "def _inverse_log(yhat: np.ndarray) -> np.ndarray:\n",
    "    return np.exp(yhat) - LOG_TOL\n",
    "\n",
    "def _classify_temporal_resolution(train_df: pd.DataFrame) -> str:\n",
    "    if len(train_df) < 3:\n",
    "        return 'unknown'\n",
    "    deltas = np.diff(train_df['ds'].sort_values().values).astype('timedelta64[D]').astype(int)\n",
    "    med = np.median(deltas)\n",
    "    if med <= 2: return 'daily'\n",
    "    if med <= 10: return 'weekly'\n",
    "    if med <= 40: return 'monthly'\n",
    "    if med <= 120: return 'quarterly'\n",
    "    return 'yearly'\n",
    "\n",
    "# Iterate datasets\n",
    "for name, df in datasets.items():\n",
    "    if len(df) < fdw + holdout_window + forecast_window + 1:\n",
    "        continue\n",
    "\n",
    "    series = _prepare_series(df)\n",
    "    if len(series) <= holdout_window + MIN_TRAIN_POINTS:\n",
    "        continue\n",
    "\n",
    "    train_end = len(series) - holdout_window\n",
    "    train_df = series.iloc[:train_end].copy()\n",
    "    holdout_df = series.iloc[train_end:].copy()\n",
    "\n",
    "    inferred_freq = pd.infer_freq(train_df['ds'])\n",
    "    temporal_res = _classify_temporal_resolution(train_df)\n",
    "\n",
    "    local_enable_monthly = ENABLE_MONTHLY\n",
    "    local_enable_quarterly = ENABLE_QUARTERLY\n",
    "\n",
    "    if temporal_res == 'yearly':\n",
    "        freq = 'YS'\n",
    "        local_enable_monthly = False\n",
    "        local_enable_quarterly = False\n",
    "        effective_holdout = min(holdout_window, 10)\n",
    "    else:\n",
    "        freq = inferred_freq or 'MS'\n",
    "        effective_holdout = holdout_window\n",
    "\n",
    "    if ROBUST_TREND:\n",
    "        train_df = train_df.copy()\n",
    "        train_df['y'] = _winsorize_outliers(train_df['y'], z=OUTLIER_Z)\n",
    "\n",
    "    train_df, used_log = _maybe_log_transform(train_df)\n",
    "    if used_log:\n",
    "        holdout_df_t = holdout_df.copy()\n",
    "        holdout_df_t['y'] = np.log(holdout_df_t['y'] + LOG_TOL)\n",
    "    else:\n",
    "        holdout_df_t = holdout_df\n",
    "\n",
    "    if temporal_res == 'yearly':\n",
    "        yearly_orders = [5, 10]\n",
    "    else:\n",
    "        yearly_orders = HYPERPARAM_GRID['yearly_fourier']\n",
    "\n",
    "    cps_list = HYPERPARAM_GRID['changepoint_prior_scale']\n",
    "    sps_list = HYPERPARAM_GRID['seasonality_prior_scale']\n",
    "    cpr_list = HYPERPARAM_GRID['changepoint_range']\n",
    "    smode_list = HYPERPARAM_GRID['seasonality_mode']\n",
    "    growth_list = HYPERPARAM_GRID['growth']\n",
    "\n",
    "    best_mae = np.inf\n",
    "    best_cfg = None\n",
    "    best_pred = None\n",
    "\n",
    "    for cps, sps, yf, cpr, smode, growth in product(cps_list, sps_list, yearly_orders, cpr_list, smode_list, growth_list):\n",
    "        # Skip logistic if we applied log transform (incompatible semantics)\n",
    "        if used_log and growth == 'logistic':\n",
    "            continue\n",
    "\n",
    "        # Prepare working copies for logistic caps/floors\n",
    "        if growth == 'logistic':\n",
    "            # Ensure positive range; fallback to linear if degenerate\n",
    "            y_train_vals = train_df['y'].values\n",
    "            y_min, y_max = np.min(y_train_vals), np.max(y_train_vals)\n",
    "            if y_max <= y_min + 1e-9:\n",
    "                continue\n",
    "            cap_val = y_max * LOGISTIC_CAP_MULT\n",
    "            floor_val = y_min * LOGISTIC_FLOOR_MULT if y_min > 0 else 0.0\n",
    "            train_use = train_df.copy()\n",
    "            train_use['cap'] = cap_val\n",
    "            train_use['floor'] = floor_val\n",
    "        else:\n",
    "            train_use = train_df\n",
    "\n",
    "        try:\n",
    "            m = Prophet(\n",
    "                yearly_seasonality=False,\n",
    "                weekly_seasonality=False,\n",
    "                daily_seasonality=False,\n",
    "                changepoint_prior_scale=cps,\n",
    "                seasonality_prior_scale=sps,\n",
    "                changepoint_range=cpr,\n",
    "                seasonality_mode=smode,\n",
    "                growth=growth,\n",
    "            )\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Add seasonalities\n",
    "        try:\n",
    "            m.add_seasonality(name='yearly', period=365.25, fourier_order=yf)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if local_enable_monthly and temporal_res in {'monthly','weekly','daily'}:\n",
    "            m.add_seasonality(name='monthly', period=30.5, fourier_order=6)\n",
    "        if local_enable_quarterly and temporal_res in {'monthly','weekly','daily','quarterly'}:\n",
    "            m.add_seasonality(name='quarterly', period=91.25, fourier_order=4)\n",
    "        if temporal_res in {'daily','weekly'}:\n",
    "            # Provide moderate weekly component for higher frequency data\n",
    "            try:\n",
    "                m.add_seasonality(name='weekly_custom', period=7, fourier_order=4)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            m.fit(train_use)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            future = m.make_future_dataframe(periods=effective_holdout, freq=freq)\n",
    "        except OverflowError:\n",
    "            try:\n",
    "                future = m.make_future_dataframe(periods=min(3, effective_holdout), freq=freq)\n",
    "            except Exception:\n",
    "                continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Add cap/floor to future if logistic\n",
    "        if growth == 'logistic':\n",
    "            future['cap'] = cap_val\n",
    "            future['floor'] = floor_val\n",
    "\n",
    "        try:\n",
    "            fcst = m.predict(future).set_index('ds')\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        aligned_holdout = holdout_df.iloc[:effective_holdout]\n",
    "        needed = aligned_holdout['ds']\n",
    "        if not set(needed).issubset(fcst.index):\n",
    "            continue\n",
    "\n",
    "        yhat = fcst.loc[needed, 'yhat'].values\n",
    "        actual = aligned_holdout['y'].values\n",
    "        if used_log:\n",
    "            yhat = _inverse_log(yhat)\n",
    "            actual = aligned_holdout['y'].values  # already original scale\n",
    "\n",
    "        mae = mean_absolute_error(actual, yhat)\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_cfg = {\n",
    "                'changepoint_prior_scale': cps,\n",
    "                'seasonality_prior_scale': sps,\n",
    "                'yearly_fourier_order': yf,\n",
    "                'changepoint_range': cpr,\n",
    "                'seasonality_mode': smode,\n",
    "                'growth': growth,\n",
    "                'log_transform': used_log,\n",
    "                'freq': freq,\n",
    "                'temporal_resolution': temporal_res,\n",
    "                'effective_holdout': effective_holdout,\n",
    "            }\n",
    "            best_pred = yhat\n",
    "\n",
    "    if best_pred is None:\n",
    "        print(f\"{name}: Prophet failed; skipping.\")\n",
    "        continue\n",
    "\n",
    "    eval_holdout = holdout_df.iloc[:best_cfg['effective_holdout']]\n",
    "    mape = mean_absolute_percentage_error(eval_holdout['y'].values, best_pred)\n",
    "    prophet_maes[name] = best_mae\n",
    "    prophet_mapes[name] = mape\n",
    "    prophet_details[name] = {\n",
    "        'config': best_cfg,\n",
    "        'mae': best_mae,\n",
    "        'mape': mape,\n",
    "        'n_train': len(train_df),\n",
    "        'n_holdout_used': best_cfg['effective_holdout'],\n",
    "        'temporal_resolution': temporal_res,\n",
    "    }\n",
    "\n",
    "    print(f\"{name} (Prophet MAE): {best_mae:.3f}\")\n",
    "\n",
    "if prophet_maes:\n",
    "    avg_mae = sum(prophet_maes.values()) / len(prophet_maes)\n",
    "    avg_mape = sum(prophet_mapes.values()) / len(prophet_mapes)\n",
    "    print(f\"Average Prophet MAE: {avg_mae:.3f}\")\n",
    "\n",
    "# prophet_details now holds per-series tuning information with expanded hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKtime Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified SKtime Performance (base sktime only) with full silent execution wrapper\n",
    "import logging, warnings, contextlib, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_error as sk_mae\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.compose import EnsembleForecaster, make_reduction\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    _HAS_XGB = True\n",
    "except Exception:\n",
    "    _HAS_XGB = False\n",
    "\n",
    "\n",
    "def _simple_split(df: pd.DataFrame, holdout: int):\n",
    "    df = df.sort_values('date')\n",
    "    y = pd.Series(df['value'].values, index=pd.to_datetime(df['date']))\n",
    "    if len(y) <= holdout + 5:\n",
    "        raise ValueError(\"Series too short\")\n",
    "    return y.iloc[:-holdout], y.iloc[-holdout:]\n",
    "\n",
    "\n",
    "def _make_es_grid():\n",
    "    \"\"\"Create a small, fast grid of ExponentialSmoothing models.\n",
    "\n",
    "    We keep this intentionally compact to avoid blowing up runtime.\n",
    "    Adjust `es_trends`, `es_seasonals`, `es_sp`, `damped_opts` to explore more.\n",
    "    \"\"\"\n",
    "    es_trends = ['add', None]              # try additive trend & no trend\n",
    "    es_seasonals = ['add', 'mul', None]    # additive, multiplicative, or none\n",
    "    es_sp = [12]                           # monthly-ish seasonality (adjust as needed)\n",
    "    damped_opts = [False, True]            # only meaningful if trend present\n",
    "\n",
    "    models = []\n",
    "    for tr in es_trends:\n",
    "        for seas in es_seasonals:\n",
    "            # If no seasonality, we don't actually need sp; but sktime still wants an int >=1\n",
    "            for sp in (es_sp if seas is not None else [1]):\n",
    "                for damp in damped_opts:\n",
    "                    if tr is None and damp:  # damped only valid with a trend component\n",
    "                        continue\n",
    "                    label = (\n",
    "                        f\"ETS-{tr or 'none'}-{seas or 'none'}\"\n",
    "                        f\"{'-damped' if damp else ''}-sp{sp if seas else 1}\"\n",
    "                    )\n",
    "                    try:\n",
    "                        mdl = ExponentialSmoothing(\n",
    "                            trend=tr,\n",
    "                            damped_trend=damp if tr is not None else False,\n",
    "                            seasonal=seas,\n",
    "                            sp=sp,\n",
    "                        )\n",
    "                        models.append((label, mdl))\n",
    "                    except Exception:\n",
    "                        # Skip any invalid combo silently\n",
    "                        continue\n",
    "    return models\n",
    "\n",
    "\n",
    "def run_sktime_benchmark_silent(datasets, holdout=24, ensemble_top=3, silent=True):\n",
    "    lines = []\n",
    "    result_mae = {}\n",
    "    best_model_map = {}\n",
    "    deseason_map = {}\n",
    "    detrend_map = {}\n",
    "    # Capture / suppress warnings + stdout/stderr\n",
    "    if silent:\n",
    "        outer_stdout = io.StringIO()\n",
    "        outer_stderr = io.StringIO()\n",
    "        warn_ctx = warnings.catch_warnings()\n",
    "        warn_ctx.__enter__()\n",
    "        warnings.simplefilter(\"ignore\")  # blanket ignore all warnings inside block\n",
    "        out_redirect = contextlib.redirect_stdout(outer_stdout)\n",
    "        err_redirect = contextlib.redirect_stderr(outer_stderr)\n",
    "        out_redirect.__enter__()\n",
    "        err_redirect.__enter__()\n",
    "    try:\n",
    "        for name, df in datasets.items():\n",
    "            if len(df) <= holdout + 25:\n",
    "                continue\n",
    "            try:\n",
    "                y_tr, y_ho = _simple_split(df, holdout)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if len(y_tr) < 30:\n",
    "                continue\n",
    "            fh = ForecastingHorizon(y_ho.index, is_relative=False)\n",
    "\n",
    "            # === Deseasonalize & Detrend (minimal addition) ===\n",
    "            y_tr_model = y_tr\n",
    "            deseason = None\n",
    "            detrender = None\n",
    "            deseason_reason = \"\"\n",
    "            detrend_reason = \"\"\n",
    "            try:\n",
    "                if len(y_tr) >= 12:\n",
    "                    try:\n",
    "                        deseason = Deseasonalizer()\n",
    "                        y_tr_model = deseason.fit_transform(y_tr_model)\n",
    "                        deseason_reason = \"applied\"\n",
    "                    except Exception:\n",
    "                        deseason = None\n",
    "                        deseason_reason = \"fail\"\n",
    "                else:\n",
    "                    deseason_reason = \"short\"\n",
    "                try:\n",
    "                    detrender = Detrender(forecaster=PolynomialTrendForecaster(degree=1))\n",
    "                    y_tr_model = detrender.fit_transform(y_tr_model)\n",
    "                    detrend_reason = \"applied\"\n",
    "                except Exception:\n",
    "                    detrender = None\n",
    "                    detrend_reason = \"fail\"\n",
    "            except Exception:\n",
    "                y_tr_model = y_tr\n",
    "                deseason = None\n",
    "                detrender = None\n",
    "                if not deseason_reason:\n",
    "                    deseason_reason = \"fail\"\n",
    "                if not detrend_reason:\n",
    "                    detrend_reason = \"fail\"\n",
    "\n",
    "            # Heuristic seasonal period guess for seasonal naive\n",
    "            sp_guess = 12 if len(y_tr) >= 24 else 1\n",
    "            try:\n",
    "                inf_freq = pd.infer_freq(y_tr.index)\n",
    "                if inf_freq:\n",
    "                    if inf_freq.startswith('W'):\n",
    "                        # weekly data\n",
    "                        sp_guess = 52 if len(y_tr) >= 104 else max(2, min(26, len(y_tr)//2))\n",
    "                    elif inf_freq[0] == 'M':\n",
    "                        sp_guess = 12\n",
    "                    elif inf_freq.startswith('Q'):\n",
    "                        sp_guess = 4\n",
    "                    elif inf_freq[0] == 'D':\n",
    "                        sp_guess = 7 if len(y_tr) >= 14 else 1\n",
    "                if sp_guess > len(y_tr)//2:\n",
    "                    sp_guess = max(1, len(y_tr)//4)\n",
    "            except Exception:\n",
    "                sp_guess = 12 if len(y_tr) >= 24 else 1\n",
    "            if sp_guess < 1:\n",
    "                sp_guess = 1\n",
    "\n",
    "            # Base (non-ES) models + Seasonal Naive\n",
    "            base_models = [\n",
    "                (\"Naive-last\", NaiveForecaster(strategy=\"last\")),\n",
    "                (\"Naive-drift\", NaiveForecaster(strategy=\"drift\")),\n",
    "                (\"Naive-seasonal\", NaiveForecaster(strategy=\"seasonal_last\", sp=sp_guess)),  # seasonal naive\n",
    "                (\"Theta\", ThetaForecaster(sp=12)),  # sp=12 seasonal period\n",
    "                (\"Trend1\", PolynomialTrendForecaster(degree=1)),\n",
    "                (\"Trend2\", PolynomialTrendForecaster(degree=2)),\n",
    "                (\"AutoARIMA\", AutoARIMA(suppress_warnings=True, maxiter=25)),\n",
    "                (\"AutoETS\", AutoETS(auto=True)),\n",
    "            ]\n",
    "\n",
    "            # Add ES hyperparameter grid models\n",
    "            es_models = _make_es_grid()\n",
    "\n",
    "            # Add Reduction models (tree/boost regressors)\n",
    "            reduction_models = []\n",
    "            try:\n",
    "                # Window lengths scale with series length but capped\n",
    "                rf_win = min(48, max(6, len(y_tr)//10))\n",
    "                reduction_models.append(\n",
    "                    (\"RF-Reduction\", make_reduction(\n",
    "                        RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1),\n",
    "                        strategy=\"recursive\",\n",
    "                        window_length=rf_win,\n",
    "                    ))\n",
    "                )\n",
    "                gb_win = min(36, max(6, len(y_tr)//12))\n",
    "                reduction_models.append(\n",
    "                    (\"GB-Reduction\", make_reduction(\n",
    "                        GradientBoostingRegressor(random_state=42),\n",
    "                        strategy=\"recursive\",\n",
    "                        window_length=gb_win,\n",
    "                    ))\n",
    "                )\n",
    "                if _HAS_XGB:\n",
    "                    xgb_win = min(48, max(6, len(y_tr)//10))\n",
    "                    reduction_models.append(\n",
    "                        (\"XGB-Reduction\", make_reduction(\n",
    "                            XGBRegressor(\n",
    "                                n_estimators=400,\n",
    "                                max_depth=4,\n",
    "                                learning_rate=0.05,\n",
    "                                subsample=0.9,\n",
    "                                colsample_bytree=0.8,\n",
    "                                objective='reg:squarederror',\n",
    "                                random_state=42,\n",
    "                                n_jobs=-1,\n",
    "                                verbosity=0,\n",
    "                            ),\n",
    "                            strategy=\"recursive\",\n",
    "                            window_length=xgb_win,\n",
    "                        ))\n",
    "                    )\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            models = base_models + es_models + reduction_models\n",
    "\n",
    "            scores = []\n",
    "            for lbl, mdl in models:\n",
    "                try:\n",
    "                    mdl.fit(y_tr_model)\n",
    "                    pred = mdl.predict(fh)\n",
    "                    # Inverse transforms (trend first, then season) if applied\n",
    "                    try:\n",
    "                        if detrender is not None:\n",
    "                            pred = detrender.inverse_transform(pred)\n",
    "                        if deseason is not None:\n",
    "                            pred = deseason.inverse_transform(pred)\n",
    "                    except Exception:\n",
    "                        pass  # if inversion fails, keep current pred\n",
    "                    mae = sk_mae(y_ho, pred)\n",
    "                    if np.isfinite(mae):\n",
    "                        scores.append((mae, lbl, mdl))\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if not scores:\n",
    "                continue\n",
    "            scores.sort(key=lambda x: x[0])\n",
    "            best_mae, best_lbl, _ = scores[0]\n",
    "\n",
    "            # Optional simple ensemble (ensemble operates on refit original transforms for consistency)\n",
    "            if ensemble_top and ensemble_top > 1 and len(scores) >= ensemble_top:\n",
    "                try:\n",
    "                    topk = scores[:ensemble_top]\n",
    "                    ens = EnsembleForecaster([(lbl, mdl) for _, lbl, mdl in topk])\n",
    "                    ens.fit(y_tr_model)\n",
    "                    e_pred = ens.predict(fh)\n",
    "                    try:\n",
    "                        if detrender is not None:\n",
    "                            e_pred = detrender.inverse_transform(e_pred)\n",
    "                        if deseason is not None:\n",
    "                            e_pred = deseason.inverse_transform(e_pred)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    e_mae = sk_mae(y_ho, e_pred)\n",
    "                    if e_mae < best_mae:\n",
    "                        best_mae = e_mae\n",
    "                        best_lbl = f\"EnsembleTop{ensemble_top}\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            result_mae[name] = float(best_mae)\n",
    "            best_model_map[name] = best_lbl\n",
    "            deseason_map[name] = deseason_reason or 'none'\n",
    "            detrend_map[name] = detrend_reason or 'none'\n",
    "            # Append transform status info\n",
    "            lines.append(\n",
    "                f\"{name}: {best_mae:.3f} (sktime {best_lbl}) [deseason={deseason_map[name]} detrend={detrend_map[name]}]\"\n",
    "            )\n",
    "    finally:\n",
    "        if silent:\n",
    "            # Exit contexts silently; discard captured text\n",
    "            err_redirect.__exit__(None, None, None)\n",
    "            out_redirect.__exit__(None, None, None)\n",
    "            warn_ctx.__exit__(None, None, None)\n",
    "\n",
    "    df_results = (\n",
    "        pd.DataFrame({\n",
    "            'dataset': list(result_mae.keys()),\n",
    "            'sktime_MAE': [result_mae[k] for k in result_mae.keys()],\n",
    "            'best_model': [best_model_map[k] for k in result_mae.keys()],\n",
    "            'deseason_status': [deseason_map[k] for k in result_mae.keys()],\n",
    "            'detrend_status': [detrend_map[k] for k in result_mae.keys()],\n",
    "        })\n",
    "        .sort_values('sktime_MAE').reset_index(drop=True)\n",
    "    )\n",
    "    return {'lines': lines, 'mae': result_mae, 'frame': df_results, 'best_model_map': best_model_map}\n",
    "\n",
    "# Run benchmark silently\n",
    "_sktime_out = run_sktime_benchmark_silent(datasets, holdout=24, ensemble_top=3, silent=True)\n",
    "sktime_results_df = _sktime_out['frame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.796111111111111"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((5.864*10) - 42.475)/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6978286262648485"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_mae = (sktime_results_df['sktime_MAE'].to_numpy().sum() - 26.994923)/12\n",
    "\n",
    "avg_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>sktime_MAE</th>\n",
       "      <th>best_model</th>\n",
       "      <th>deseason_status</th>\n",
       "      <th>detrend_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNRATE</td>\n",
       "      <td>0.176712</td>\n",
       "      <td>EnsembleTop3</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CO2</td>\n",
       "      <td>0.286098</td>\n",
       "      <td>XGB-Reduction</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INDPRO</td>\n",
       "      <td>1.337847</td>\n",
       "      <td>Naive-last</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quadratic</td>\n",
       "      <td>1.437959</td>\n",
       "      <td>Trend2</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Walk (drift)</td>\n",
       "      <td>1.479566</td>\n",
       "      <td>ETS-none-add-sp12</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Seasonal+Trend</td>\n",
       "      <td>1.566502</td>\n",
       "      <td>XGB-Reduction</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Multi-seasonal</td>\n",
       "      <td>1.569462</td>\n",
       "      <td>RF-Reduction</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Spiky Intermittent</td>\n",
       "      <td>1.655270</td>\n",
       "      <td>ETS-add-add-damped-sp12</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Linear Trend</td>\n",
       "      <td>2.145040</td>\n",
       "      <td>EnsembleTop3</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic (S-curve)</td>\n",
       "      <td>2.324673</td>\n",
       "      <td>ETS-add-none-sp1</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Piecewise (changepoints)</td>\n",
       "      <td>2.973755</td>\n",
       "      <td>ETS-add-add-sp12</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CPIAUCSL</td>\n",
       "      <td>3.421060</td>\n",
       "      <td>Naive-drift</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sunspots</td>\n",
       "      <td>26.994923</td>\n",
       "      <td>GB-Reduction</td>\n",
       "      <td>fail</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     dataset  sktime_MAE               best_model  \\\n",
       "0                     UNRATE    0.176712             EnsembleTop3   \n",
       "1                        CO2    0.286098            XGB-Reduction   \n",
       "2                     INDPRO    1.337847               Naive-last   \n",
       "3                  Quadratic    1.437959                   Trend2   \n",
       "4        Random Walk (drift)    1.479566        ETS-none-add-sp12   \n",
       "5             Seasonal+Trend    1.566502            XGB-Reduction   \n",
       "6             Multi-seasonal    1.569462             RF-Reduction   \n",
       "7         Spiky Intermittent    1.655270  ETS-add-add-damped-sp12   \n",
       "8               Linear Trend    2.145040             EnsembleTop3   \n",
       "9         Logistic (S-curve)    2.324673         ETS-add-none-sp1   \n",
       "10  Piecewise (changepoints)    2.973755         ETS-add-add-sp12   \n",
       "11                  CPIAUCSL    3.421060              Naive-drift   \n",
       "12                  Sunspots   26.994923             GB-Reduction   \n",
       "\n",
       "   deseason_status detrend_status  \n",
       "0             fail        applied  \n",
       "1             fail        applied  \n",
       "2             fail        applied  \n",
       "3             fail        applied  \n",
       "4             fail        applied  \n",
       "5             fail        applied  \n",
       "6             fail        applied  \n",
       "7             fail        applied  \n",
       "8             fail        applied  \n",
       "9             fail        applied  \n",
       "10            fail        applied  \n",
       "11            fail        applied  \n",
       "12            fail        applied  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sktime_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB forecasting block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def ts_train_test_split(\n",
    "    X: pd.DataFrame, \n",
    "    y: pd.Series, \n",
    "    outcome_col: str, \n",
    "    date_col: str, \n",
    "    fdw: int, \n",
    "    holdout_window: int,\n",
    "    forecast_window: Optional[int] = 1 \n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Apply preprocessing and split the data into training and testing sets for time series modeling.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper function to preprocess ts data\n",
    "    def _ts_preproc(inp_tbl: pd.DataFrame, inp_y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:   \n",
    "        preproc_tbl = (inp_tbl\n",
    "        .pipe(lambda x: x.assign(**{f\"lagged_{outcome_col}_{i}m\": x[outcome_col].shift(i) for i in range(forecast_window, fdw + 1)}))\n",
    "        .pipe(lambda x: x.assign(**{f\"inv_hyp_sin_lagged_{outcome_col}_{i}m\": np.arcsinh(x[outcome_col].shift(i)) for i in range(forecast_window, fdw + 1)}))\n",
    "        .pipe(lambda x: x.assign(**{f\"rolling_avg_{outcome_col}_{i}m\": x[outcome_col].shift(1).rolling(window=i).mean() for i in range(forecast_window, fdw + 1)}))\n",
    "        .pipe(lambda x: x.assign(**{f\"min_{outcome_col}_{i}m\": x[outcome_col].shift(1).rolling(window=i).min() for i in range(forecast_window, fdw + 1)}))\n",
    "        # New time and seasonal features\n",
    "        .pipe(lambda x: x.assign(\n",
    "            # t=np.arange(len(x)),\n",
    "            monthsin=np.sin(2 * np.pi * pd.to_datetime(x[date_col]).dt.month / 12.0),\n",
    "            monthcos=np.cos(2 * np.pi * pd.to_datetime(x[date_col]).dt.month / 12.0),\n",
    "        ))\n",
    "        # Drop the original date and outcome columns\n",
    "        .drop([date_col, outcome_col], axis=1)\n",
    "        # Rowwise deletion of missing values\n",
    "        .dropna(axis=0)\n",
    "        )\n",
    "        preproc_y = inp_y.loc[preproc_tbl.index]\n",
    "\n",
    "        return preproc_tbl, preproc_y\n",
    "\n",
    "    # Reset index of X and y\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Calculate the index to split the data\n",
    "    train_end_index = X.shape[0] - (holdout_window)\n",
    "    test_start_index = X.shape[0] - (fdw + holdout_window)\n",
    "\n",
    "    # Split the data\n",
    "    X_train = X.iloc[:train_end_index]\n",
    "    X_test = X.iloc[test_start_index:]\n",
    "    y_train = y.iloc[:train_end_index]\n",
    "    y_test = y.iloc[test_start_index:]\n",
    "\n",
    "    # Set the indices of both X and y train/test to the 'date' column \n",
    "    X_train.set_index(date_col, drop=False, inplace=True)\n",
    "    y_train.index = X_train.index\n",
    "    X_test.set_index(date_col, drop=False, inplace=True)\n",
    "    y_test.index = X_test.index\n",
    "\n",
    "    # Preprocess the data\n",
    "    X_train, y_train = _ts_preproc(X_train, y_train)\n",
    "    X_test, y_test = _ts_preproc(X_test, y_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from automl_tool.estimation import XGBWithEarlyStoppingRegressor\n",
    "from pandas_datareader import data as pdr\n",
    "from automl_tool.preprocessing import ts_train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np \n",
    "\n",
    "fdw, holdout_window, forecast_window = 12, 24, 1\n",
    "\n",
    "df_fred = pdr.DataReader(\"CPIAUCSL\", 'fred', start='1990-01-01')\n",
    "df_fred = df_fred.rename(columns={'CPIAUCSL': 'value'}).reset_index().rename(columns={'DATE': 'date'})\n",
    "df_fred = df_fred.dropna()\n",
    "\n",
    "X, y = df_fred, df_fred['value']\n",
    "X_train, X_holdout, y_train, y_holdout = ts_train_test_split(\n",
    "\tX, y, 'value', 'date', fdw, holdout_window, forecast_window=forecast_window\n",
    ")\n",
    "\n",
    "xgb_model = XGBWithEarlyStoppingRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "preds = xgb_model.predict(X_holdout)\n",
    "mae = mean_absolute_error(y_holdout, preds)\n",
    "print(f\"XGBRegressor MAE on CPIAUCSL: {mae:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoML forecasting block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl_tool.automl import AutoML\n",
    "\n",
    "automl_mod = AutoML(X_train, y_train, 'value', time_series=True)\n",
    "\n",
    "automl_mod.fit_pipeline(holdout_window=holdout_window)\n",
    "\n",
    "# Get the best model from the fitted pipeline\n",
    "y_preds = automl_mod.fitted_pipeline.predict(X_holdout)\n",
    "\n",
    "mean_absolute_error(y_holdout, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot actual and predicts on holdout set \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_holdout.index, y_holdout, label='Actual', color='blue')\n",
    "plt.plot(y_holdout.index, y_preds, label='Predicted', color='orange')\n",
    "legend = plt.legend(loc='upper left', fontsize=12)\n",
    "plt.title('Predictions vs Actuals on Holdout Set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_mod.fitted_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalecast forecasting block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from scalecast.Forecaster import Forecaster\n",
    "from scalecast import GridGenerator\n",
    "\n",
    "GridGenerator.get_example_grids()  # example hyperparameter grids\n",
    "\n",
    "data = df_fred\n",
    "f = Forecaster(\n",
    "    y=data['value'],               # required\n",
    "    current_dates=data['date'],    # required\n",
    "    future_dates=1,               # length of the forecast horizon\n",
    "    test_length=24,                 # set a test set length or fraction to validate all models if desired\n",
    "    cis=False,                     # choose whether or not to evaluate confidence intervals for all models\n",
    ")\n",
    "f.set_estimator('xgboost')  # select an estimator\n",
    "\n",
    "f.auto_Xvar_select()       # find best look-back, trend, and seasonality for your series\n",
    "f.cross_validate(k=3)       # tune model hyperparams using time series cross validation\n",
    "f.auto_forecast()           # automatically forecast with the chosen Xvars and hyperparams\n",
    "\n",
    "results = f.export(['lvl_fcsts','model_summaries'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_preds = f.export('lvl_test_set_predictions')\n",
    "\n",
    "mean_absolute_error(ts_preds['actual'], ts_preds['xgboost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_holdout.index, ts_preds['actual'], label='Actual', color='blue')\n",
    "plt.plot(y_holdout.index, ts_preds['xgboost'], label='Predicted', color='orange')\n",
    "legend = plt.legend(loc='upper left', fontsize=12)\n",
    "plt.title('XGBWithEarlyStoppingRegressor Predictions vs Actuals on Holdout Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scalecast import GridGenerator\n",
    "GridGenerator.get_example_grids()   # writes Grids.py to your working dir (if not already present)\n",
    "\n",
    "# then either open Grids.py in your editor, or import it:\n",
    "from Grids import xgboost as xgb_grid\n",
    "print(xgb_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['model_summaries']['HyperParams'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
